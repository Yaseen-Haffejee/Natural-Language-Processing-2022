{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7226a699",
   "metadata": {},
   "source": [
    "### General Instructions\n",
    "\n",
    "Do not change the file name, method name or any variable name in your submission file. \n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and student number below.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Also, ensure that your notebook does not give errors before submitting. Ensure there is no 'Assertion Error', 'NotImplementedError' or  test(s) failed  feedback. \n",
    "\n",
    "NotImplementedError: this means there is a code cell/task you are yet to implement.\n",
    "\n",
    "AssertionError: this means your implementation is failing some tests.\n",
    "\n",
    "Note that your assignment will be checked with additional test cases after submission. Ensure you work with the instructions given\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bea64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Yaseen Haffejee\"\n",
    "STUDENT_NUMBER = \"1827555\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdeb441",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34a78d",
   "metadata": {},
   "source": [
    "# <center>  COMS4054A/COMS7066A </center>\n",
    "# <center> Natural Language Processing/Technology (NLP) 2022 </center>\n",
    "## <center> Lab Session 3 </center>\n",
    "### <center> 18 August, 2022 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c3a13",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "#### Total (30 points/marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e26b95",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "1. To practice the concepts learnt in class\n",
    "1. To build a simple autocomplete system\n",
    "1. To use ngram models to generate random text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ff676d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b701ff19c381b9915a9b0d8f95488c15",
     "grade": false,
     "grade_id": "cell-c20ec8e85b92870c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#RUN THIS CELL\n",
    "test1 = \"\"\"  Sentence 1.\\n   Sentence 2.\\n Sentence 3.  \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461f98a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7a7cce550442c652121fd3a0bedd2b",
     "grade": false,
     "grade_id": "cell-2f69997df1b77620",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1 - Sentence Segmentation  (2 points)\n",
    "1. Split the data using \"\\n\".\n",
    "2. Remove leading and trailing spaces.\n",
    "1. Delete sentences if they are emptystrings\n",
    "\n",
    "Expected Output:\n",
    "![title](test1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e029caa6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb37d5f3596911386277a57586584f70",
     "grade": false,
     "grade_id": "Task1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sentence_segmentation(data):\n",
    "    \"\"\"\n",
    "    perform sentence segmentation using the linebreak \"\\n\" \n",
    "    \n",
    "    Returns: A list of sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    # Splitting by the newline character\n",
    "    split_data = data.split(\"\\n\")\n",
    "    # Removing empty strings\n",
    "    sentences = [x for x in split_data if x!= \"\"]\n",
    "    # Removing the leading and trailing spaces\n",
    "    sentences = [x.strip(\" \") for x in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f306cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence 1.', 'Sentence 2.', 'Sentence 3.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_segmentation(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d926a7e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c791163cb45bc4beddb3c60469b1225",
     "grade": true,
     "grade_id": "Task1test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert sentence_segmentation(test1) == ['Sentence 1.', 'Sentence 2.', 'Sentence 3.'], \"Task 1 test(s) failed!\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de2a532",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5d44bbb2771067c7adffdbe30f710ec",
     "grade": true,
     "grade_id": "Task1hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe77ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f76baee2916347d1c40091024258c31d",
     "grade": false,
     "grade_id": "cell-e3bc5feb31f3d176",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2 - Tokenization (2 points)\n",
    "1. Perform case folding of text into lowercase\n",
    "1. Use the nltk.word_tokenize method to split each sentence into a list of tokens\n",
    "1. Append each set of tokenized sentences into a list of tokenized sentences.\n",
    "\n",
    "Expected Output:\n",
    "![title](test2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b36aa5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ebaec21e1465c86b3bb67c681c0cb28",
     "grade": false,
     "grade_id": "cell-f6a2d434e7e70575",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8460a29",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b7ff3f2d95c5c69ee5d59409ec02783",
     "grade": false,
     "grade_id": "Task2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"Args:\n",
    "    sentences: List of strings\n",
    "    returns: list of list of tokens\"\"\"\n",
    "    \n",
    "    # Make everything lowercase\n",
    "    sentences = [x.lower() for x in sentences]\n",
    "    # Tokenize the sentences\n",
    "    tokenized_sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b836f1c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9f1f6650dcbac79633bb1c0661ee4e3",
     "grade": true,
     "grade_id": "Task2test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert tokenize_sentences(sentence_segmentation(test1)) == [['sentence', '1', '.'], ['sentence', '2', '.'], ['sentence', '3', '.']], \"Task2 test(s) failed!\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c606858d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fe10ba623b2c0cc1459b8eaaa15168f",
     "grade": true,
     "grade_id": "Task2hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2997242",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43820cd1d3edd3f9c06d816af2dd2e05",
     "grade": false,
     "grade_id": "cell-fb82ff90a0fb9414",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3 - Count words (2 points)\n",
    "1. Count all the appearances of a token in the data\n",
    "2. Create and return a dictionary that maps each token with its count/frequency.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "![title](test3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcd3241",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bd01ec8b153a51c683aa2a1a5b43f33",
     "grade": false,
     "grade_id": "Task3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_tokens(tokenized_sentences):\n",
    "    \"\"\"Args: list of list of strings (tokens)\n",
    "    Returns: a dictionary that maps a token (str) to its frequency (int)\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Create a single list of all the tokens\n",
    "    all_words = [word for sublist in tokenized_sentences for word in sublist]\n",
    "    word_counts = {}\n",
    "    # Get all the tokens and their frequencies\n",
    "    for word in all_words:\n",
    "        try:\n",
    "            word_counts[word] = word_counts[word] + 1\n",
    "        except:\n",
    "            word_counts[word] = 1\n",
    "            \n",
    "    # Create a dictionary of the token and its frequency\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e0c1fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9f15261711906acc8914b32c6b63a8d",
     "grade": true,
     "grade_id": "Task3test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert count_tokens(tokenize_sentences(sentence_segmentation(test1))) == {'sentence': 3, '1': 1, '.': 3, '2': 1, '3': 1}, \"Task 3 test(s) failed!\"\n",
    "print(\"Test(s) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "618d6005",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d59eb2c5440d02e0a192914600e459ef",
     "grade": true,
     "grade_id": "Task3hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d4040e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 3, '1': 1, '.': 3, '2': 1, '3': 1}\n"
     ]
    }
   ],
   "source": [
    "print( count_tokens(tokenize_sentences(sentence_segmentation(test1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c5b5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eaf9a8e01e3923825faa47285fde151d",
     "grade": false,
     "grade_id": "cell-985bf64308bf95ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Out of vocabulary words\n",
    "\n",
    "Often, we do not use all the tokens that occur in the data for training, we use frequently occuring words.  Given the words in our training set, we can decide all words that occur a certain number of times would be included in our vocabulary and used in the training. \n",
    "\n",
    "So what happens to the other words, a way to handle them is to represent them with an unknown word token.\n",
    "\n",
    "This way the training data also has some unknown words to train on. If in the test data, we encounter unknown words, we'll use the information we already have about unknown words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9684c7",
   "metadata": {},
   "source": [
    "### Task 4 - Vocabulary (2 points)\n",
    "\n",
    "Write a function such that given a frequency_threshold, returns the list of words that meet the given threshold (equal to  or greater than the threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba68d5a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08addd67e4d71497d04c9d2038793697",
     "grade": false,
     "grade_id": "Task4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_words_with_frequency(tokenized_sentences, frequency_threshold):\n",
    "    \"\"\"Args:\n",
    "    tokenized sentences: list of list of strings\n",
    "    frequency_threshold: minimum no of occurrences for a word to be in the vocabulary\n",
    "    \n",
    "    returns: a list of words that meet the given threshold.\"\"\"\n",
    "    \n",
    "    vocabulary = []\n",
    "    # YOUR CODE HERE\n",
    "    # Get the tokens and their frequencies using the count_tokens method\n",
    "    tokens_frequencies = count_tokens(tokenized_sentences)\n",
    "    # Use the threshold to remove any token with count less than the threshold\n",
    "    for key,value in tokens_frequencies.items():\n",
    "        if(value>= frequency_threshold):\n",
    "            vocabulary.append(key)\n",
    "            \n",
    "    return vocabulary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "385b4d74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb90f50cef5c8e67758d5cf289012b3f",
     "grade": false,
     "grade_id": "cell-b8f060c9b748fb18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#RUN THIS CELL\n",
    "tokenized_sentences =[['sentence', '1', '.'], ['sentence', '2', '.'], ['sentence', '3', '.']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "180cad32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e367f4a3d0aec2bb5f2df2bbd2f71d1e",
     "grade": true,
     "grade_id": "Task4_test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert get_words_with_frequency(tokenized_sentences,3) == ['sentence', '.'], \"Task 4 test(s) failed!\"\n",
    "print(\"Test(s) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e02976e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ccd54026472f45bb52617ade5467b7f",
     "grade": true,
     "grade_id": "Task4_testhidden",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ebc55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d18ff66419e8b77f99d2462a991bca7",
     "grade": false,
     "grade_id": "cell-c5c1de77e288e092",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 5 (2 points)\n",
    "Given a list of tokenized sentences, replace all words that are not in the given vocabulary with the \"\\<unk\\>\" token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5413ff83",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95175de85d50b0b21e56099d985e68c4",
     "grade": false,
     "grade_id": "Task5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def replace_unknown_words(tokenized_sentences, vocabulary, unknown_token='<unk>'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    tokenized_sentences = list of lists of strings\n",
    "    vocabulary = list of recognized strings\n",
    "    unknown_token = string to represent out-of-vocabulary words\n",
    "    \n",
    "    Returns:\n",
    "        A list of lists of strings, with words not in the vocabulary replaced\n",
    "        \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        temp = []\n",
    "        for word in sentence:\n",
    "            if word in vocabulary:\n",
    "                temp.append(word)\n",
    "            else: \n",
    "                temp.append(\"<unk>\")\n",
    "        replaced_tokenized_sentences.append(temp)\n",
    "\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c59ef7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "237b091f4e8abe525751a4550ac93abd",
     "grade": true,
     "grade_id": "Task5test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "vocabulary = ['sentence', '.']\n",
    "assert replace_unknown_words(tokenized_sentences, vocabulary, unknown_token='<unk>') == [['sentence', '<unk>', '.'], ['sentence', '<unk>', '.'], ['sentence', '<unk>', '.']], \"Task 5 test(s) failed!\"\n",
    "print(\"Test(s) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abc8bb49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47fad18833944720674784828fdee3ef",
     "grade": true,
     "grade_id": "Task5hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611eac04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a64879899b8a5c450d39e94aa0283916",
     "grade": false,
     "grade_id": "cell-5b8a6e0f0da4ade6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 6 (2 points)\n",
    "Calculate all the n-grams that occur in a dataset and the number of times they occur.\n",
    "\n",
    "The data is presented as a list of list of strings, where each nested list represents a sentence. \n",
    "\n",
    "Add 'n' start tokens and 1 end token to each sentence before the calculation.\n",
    "(Although for a n-gram you need to use n-1 starting markers for a sentence, for this implemtation, prepend 'n' starting markers in order to use them to compute the initial probability for the LM later in the assignment.)\n",
    "\n",
    "Your function should return a dictionary that maps a tuple of n-words to its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df9807e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25827deeabe1c63558842d3d99dd1e9f",
     "grade": false,
     "grade_id": "cell-48a08396516d9b47",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "    new_data = []\n",
    "    # Adding the start and end tokens to the sentences\n",
    "    for sentence in data:\n",
    "        new_sentence = [start_token]*n\n",
    "        new_sentence.extend(sentence)\n",
    "        new_sentence.append(end_token)\n",
    "        new_data.append(new_sentence)\n",
    "\n",
    "    # Doing the counting of the n-grams\n",
    "    for sentence in new_data:\n",
    "        length = len(sentence)\n",
    "        for i in range(length-1):\n",
    "            key = tuple(sentence[i:i+n])\n",
    "            try:\n",
    "                n_grams[key] = n_grams[key] + 1\n",
    "\n",
    "            except:\n",
    "                n_grams[key] = 1\n",
    "\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7be8ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eac5ba5726e0d21e6e288091330efb2",
     "grade": false,
     "grade_id": "cell-deda7e6548c6590a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Expected Output\n",
    "Bi-gram:\n",
    "\n",
    "{('\\<s\\>', '\\<s\\>'): 2, ('\\<s\\>', 'i'): 1, ('i', 'like'): 1, ('like', 'green'): 1, ('green', 'apples'): 1, \n",
    " ('apples', '\\<e\\>'): 1, ('\\<s\\>', 'this'): 1, ('this', 'apple'): 1, ('apple', 'looks'): 1, ('looks', 'like'): 1, \n",
    " ('like', 'an'): 1, ('an', 'orange'): 1, ('orange', '\\<e\\>'): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df1a9a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('green',): 1, ('apples',): 1, ('this',): 1, ('apple',): 1, ('looks',): 1, ('an',): 1, ('orange',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'green'): 1, ('green', 'apples'): 1, ('apples', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'apple'): 1, ('apple', 'looks'): 1, ('looks', 'like'): 1, ('like', 'an'): 1, ('an', 'orange'): 1, ('orange', '<e>'): 1}\n",
      "Tri-gram:\n",
      "{('<s>', '<s>', '<s>'): 2, ('<s>', '<s>', 'i'): 1, ('<s>', 'i', 'like'): 1, ('i', 'like', 'green'): 1, ('like', 'green', 'apples'): 1, ('green', 'apples', '<e>'): 1, ('apples', '<e>'): 1, ('<s>', '<s>', 'this'): 1, ('<s>', 'this', 'apple'): 1, ('this', 'apple', 'looks'): 1, ('apple', 'looks', 'like'): 1, ('looks', 'like', 'an'): 1, ('like', 'an', 'orange'): 1, ('an', 'orange', '<e>'): 1, ('orange', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'green', 'apples'],\n",
    "             ['this', 'apple', 'looks', 'like', 'an', 'orange']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))\n",
    "print(\"Tri-gram:\")\n",
    "print(count_n_grams(sentences, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6342b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 3, ('I',): 3, ('am',): 2, ('Sam',): 2, ('do',): 1, ('not',): 1, ('like',): 1, ('green',): 1, ('eggs',): 1, ('and',): 1, ('ham',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 3, ('<s>', 'I'): 2, ('I', 'am'): 2, ('am', 'Sam'): 1, ('Sam', '<e>'): 1, ('<s>', 'Sam'): 1, ('Sam', 'I'): 1, ('am', '<e>'): 1, ('I', 'do'): 1, ('do', 'not'): 1, ('not', 'like'): 1, ('like', 'green'): 1, ('green', 'eggs'): 1, ('eggs', 'and'): 1, ('and', 'ham'): 1, ('ham', '<e>'): 1}\n",
      "Tri-gram:\n",
      "{('<s>', '<s>', '<s>'): 3, ('<s>', '<s>', 'I'): 2, ('<s>', 'I', 'am'): 1, ('I', 'am', 'Sam'): 1, ('am', 'Sam', '<e>'): 1, ('Sam', '<e>'): 1, ('<s>', '<s>', 'Sam'): 1, ('<s>', 'Sam', 'I'): 1, ('Sam', 'I', 'am'): 1, ('I', 'am', '<e>'): 1, ('am', '<e>'): 1, ('<s>', 'I', 'do'): 1, ('I', 'do', 'not'): 1, ('do', 'not', 'like'): 1, ('not', 'like', 'green'): 1, ('like', 'green', 'eggs'): 1, ('green', 'eggs', 'and'): 1, ('eggs', 'and', 'ham'): 1, ('and', 'ham', '<e>'): 1, ('ham', '<e>'): 1}\n",
      "Quad-gram:\n",
      "{('<s>', '<s>', '<s>', '<s>'): 3, ('<s>', '<s>', '<s>', 'I'): 2, ('<s>', '<s>', 'I', 'am'): 1, ('<s>', 'I', 'am', 'Sam'): 1, ('I', 'am', 'Sam', '<e>'): 1, ('am', 'Sam', '<e>'): 1, ('Sam', '<e>'): 1, ('<s>', '<s>', '<s>', 'Sam'): 1, ('<s>', '<s>', 'Sam', 'I'): 1, ('<s>', 'Sam', 'I', 'am'): 1, ('Sam', 'I', 'am', '<e>'): 1, ('I', 'am', '<e>'): 1, ('am', '<e>'): 1, ('<s>', '<s>', 'I', 'do'): 1, ('<s>', 'I', 'do', 'not'): 1, ('I', 'do', 'not', 'like'): 1, ('do', 'not', 'like', 'green'): 1, ('not', 'like', 'green', 'eggs'): 1, ('like', 'green', 'eggs', 'and'): 1, ('green', 'eggs', 'and', 'ham'): 1, ('eggs', 'and', 'ham', '<e>'): 1, ('and', 'ham', '<e>'): 1, ('ham', '<e>'): 1}\n",
      "Quint-gram:\n",
      "{('<s>', '<s>', '<s>', '<s>', '<s>'): 3, ('<s>', '<s>', '<s>', '<s>', 'I'): 2, ('<s>', '<s>', '<s>', 'I', 'am'): 1, ('<s>', '<s>', 'I', 'am', 'Sam'): 1, ('<s>', 'I', 'am', 'Sam', '<e>'): 1, ('I', 'am', 'Sam', '<e>'): 1, ('am', 'Sam', '<e>'): 1, ('Sam', '<e>'): 1, ('<s>', '<s>', '<s>', '<s>', 'Sam'): 1, ('<s>', '<s>', '<s>', 'Sam', 'I'): 1, ('<s>', '<s>', 'Sam', 'I', 'am'): 1, ('<s>', 'Sam', 'I', 'am', '<e>'): 1, ('Sam', 'I', 'am', '<e>'): 1, ('I', 'am', '<e>'): 1, ('am', '<e>'): 1, ('<s>', '<s>', '<s>', 'I', 'do'): 1, ('<s>', '<s>', 'I', 'do', 'not'): 1, ('<s>', 'I', 'do', 'not', 'like'): 1, ('I', 'do', 'not', 'like', 'green'): 1, ('do', 'not', 'like', 'green', 'eggs'): 1, ('not', 'like', 'green', 'eggs', 'and'): 1, ('like', 'green', 'eggs', 'and', 'ham'): 1, ('green', 'eggs', 'and', 'ham', '<e>'): 1, ('eggs', 'and', 'ham', '<e>'): 1, ('and', 'ham', '<e>'): 1, ('ham', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['I', 'am', 'Sam'],\n",
    "             ['Sam', 'I', 'am'],\n",
    "             [\"I\", \"do\", \"not\", \"like\", \"green\", 'eggs', 'and', 'ham' ]]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))\n",
    "print(\"Tri-gram:\")\n",
    "print(count_n_grams(sentences, 3))\n",
    "print(\"Quad-gram:\")\n",
    "print(count_n_grams(sentences, 4))\n",
    "print(\"Quint-gram:\")\n",
    "print(count_n_grams(sentences, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90d94b4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "857d0c286a05dbad022dff60b2411fd4",
     "grade": true,
     "grade_id": "Task6test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "sentences = [['I', 'am', 'Sam'],\n",
    "             ['Sam', 'I', 'am'],\n",
    "             [\"I\", \"do\", \"not\", \"like\", \"green\", 'eggs', 'and', 'ham' ]]\n",
    "\n",
    "assert count_n_grams(sentences, 2) == {('<s>', '<s>'): 3, ('<s>', 'I'): 2, ('I', 'am'): 2, ('am', 'Sam'): 1, ('Sam', '<e>'): 1, ('<s>', 'Sam'): 1, ('Sam', 'I'): 1, ('am', '<e>'): 1, ('I', 'do'): 1, ('do', 'not'): 1, ('not', 'like'): 1, ('like', 'green'): 1, ('green', 'eggs'): 1, ('eggs', 'and'): 1, ('and', 'ham'): 1, ('ham', '<e>'): 1}, \"Task 6 test(s) failed!\"\n",
    "print(\"Test(s) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac0f2cbc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2deeca9e776dc7d54a439117f0d12da6",
     "grade": true,
     "grade_id": "Task6hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed23dfa",
   "metadata": {},
   "source": [
    "### Task 7 (2 points)\n",
    "Let's estimate the n-gram probabilities using the formula:\n",
    "\n",
    "![title](image.png)\n",
    "\n",
    "Write a function that returns the n-gram probabilities, given the n-gram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f5cf7c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01009f919bbba0f65327e5525803b125",
     "grade": false,
     "grade_id": "Task7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_probability_raw(word, n_minus_1_gram, \n",
    "                         n_minus_1_gram_counts, n_gram_counts, vocabulary_size):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        word: next word (estimating its probability)\n",
    "        n_minus_1_gram: A sequence of words of length n-1\n",
    "        n_minus_1_gram_counts: Dictionary of counts of (n-1)-grams\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    n_minus_1_gram = tuple(n_minus_1_gram)\n",
    "    \n",
    "    numerator = n_gram_counts[n_minus_1_gram+tuple([word])]\n",
    "    denominator = n_minus_1_gram_counts[n_minus_1_gram]\n",
    "    probability = numerator/denominator\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c32a6ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ca52b2b6cf893bbe5f5cfed94cbb540",
     "grade": true,
     "grade_id": "Task7_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "# Expected output = 0.33\n",
    "\n",
    "sentences = [['I', 'am', 'Sam'],\n",
    "             ['Sam', 'I', 'am'],\n",
    "             [\"I\", \"do\", \"not\", \"like\", \"green\", 'eggs', 'and', 'ham' ]]\n",
    "unique_words = list(set(sentences[0] + sentences[1]+sentences[2]))\n",
    "previous = ['am']\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "prob = estimate_probability_raw(\"do\", ['I'], unigram_counts, bigram_counts, len(unique_words))\n",
    "\n",
    "assert round(prob,2) == 0.33, \"Task 7 test(s) failed!\"\n",
    "print(\"Test(s) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5018d9fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59c4ffe26bb4c72ebf028d70b1946cfb",
     "grade": true,
     "grade_id": "Task7hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb3df9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d6ce55c0d99f4f43696cef2a8500174",
     "grade": false,
     "grade_id": "Task8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 8 - Laplace Smoothing (2 points)\n",
    "Create a function that returns the probabilities of the n-grams after perfroming add-1 smoothing using the formula:\n",
    "\n",
    "![title](image_2.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b526e03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b45c1467f772c0272eb9aeea0fc28c4",
     "grade": false,
     "grade_id": "Task8test",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_probability(word, n_minus_1_gram, \n",
    "                         n_minus_1_gram_counts, n_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        word: next word\n",
    "        n_minus_1_gram: A sequence of words of length n-1\n",
    "        n_minus_1_gram_counts: Dictionary of counts of (n-1)grams\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    n_minus_1_gram = tuple(n_minus_1_gram)\n",
    "    # We use the try and except in case an n-gram has never been seen before\n",
    "    try: \n",
    "        numerator = n_gram_counts[n_minus_1_gram+tuple([word])]\n",
    "    except:\n",
    "        numerator = 0\n",
    "    try:\n",
    "        denominator = n_minus_1_gram_counts[n_minus_1_gram]\n",
    "    except: \n",
    "        denominator = 0\n",
    "    probability = (numerator+k)/(denominator+vocabulary_size)\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3548086a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c30e5a6b323b591be5f70ba9006ee827",
     "grade": true,
     "grade_id": "Task8_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "# Expected output = 0.15\n",
    "\n",
    "sentences = [['I', 'am', 'Sam'],\n",
    "             ['Sam', 'I', 'am'],\n",
    "             [\"I\", \"do\", \"not\", \"like\", \"green\", 'eggs', 'and', 'ham' ]]\n",
    "unique_words = list(set(sentences[0] + sentences[1]+sentences[2]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "prob_laplace = estimate_probability(\"do\", ['I'], unigram_counts, bigram_counts, len(unique_words))\n",
    "assert round(prob_laplace,2) == 0.15, \"Task 8 test(s) failed!\"\n",
    "print(\"Test(s) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96c0873d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "855e58c2e2e0dc55632eda207b4b0025",
     "grade": true,
     "grade_id": "Task8_hiddentest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125ebd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e260f09bce52909050ffdaa71d60ace",
     "grade": false,
     "grade_id": "cell-06374467abaf1e6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Estimate probabilities for all words (1 point)\n",
    "The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.\n",
    "\n",
    "This function is provided for you.\n",
    "\n",
    "The expected output after running the test is:\n",
    "\n",
    "![title](image4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "874efd50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00a051853d985fee0cf88d3850aca83f",
     "grade": false,
     "grade_id": "cell-5bee3ecc64a15a77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_probabilities(n_minus_1_gram, n_minus_1_gram_counts, n_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        n_minus_1_gram: A sequence of words of length n-1\n",
    "        n_minus_1_gram_counts: Dictionary of counts of (n-1)grams\n",
    "        n_gram_counts : Dictionary of counts of (n)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    n_minus_1_gram = tuple(n_minus_1_gram)    \n",
    "    \n",
    "    # add <e> <unk> to the vocabulary\n",
    "        # <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
    "    vocabulary_size = len(vocabulary)    \n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, n_minus_1_gram, \n",
    "                                           n_minus_1_gram_counts, n_gram_counts , \n",
    "                                           vocabulary_size, k=k)\n",
    "                \n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0e0915c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcf41dcc50058c66ae44a5fd640c1bf0",
     "grade": true,
     "grade_id": "cell-83e83295465c517a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "assert estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1) == {'i': 0.09090909090909091,\n",
    " 'is': 0.09090909090909091,\n",
    " 'cat': 0.2727272727272727,\n",
    " 'this': 0.09090909090909091,\n",
    " 'a': 0.09090909090909091,\n",
    " 'like': 0.09090909090909091,\n",
    " 'dog': 0.09090909090909091,\n",
    " '<e>': 0.09090909090909091,\n",
    " '<unk>': 0.09090909090909091}, \"Task 8 Test failed\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0c89e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "487608c60bb03c01c2e4b69f7db46179",
     "grade": false,
     "grade_id": "Task9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 9 (4 points)\n",
    "Write a function to compute the probabilites of all possible next words and return the word with the highest probability, alongside its probability as a tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f9fb709",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f44b105e60b3fa78041d4d9e37e10a58",
     "grade": false,
     "grade_id": "cell-4cae473dd932f2b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens,n_minus_1_gram_counts , n_gram_counts , vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length >= n -1\n",
    "        n_minus_1_gram_counts: Dictionary of counts of (n-1)-grams\n",
    "        n_gram_counts: Dictionary of counts of (n)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ## Get n which is how many words we are considering for the history\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    n_minus_one = n-1\n",
    "    ## From the incoming words that we want to predict the next word for, we can the n-1 words\n",
    "    n_minus_1_gram = previous_tokens[-n_minus_one:]\n",
    "    ## caluclate the probability of each word in the vocab being the next word\n",
    "    probabilities = estimate_probabilities(n_minus_1_gram, n_minus_1_gram_counts, n_gram_counts, vocabulary, k=k)\n",
    "\n",
    "    if (start_with != None):\n",
    "        items_starting_with = [ (k,v) for k,v in probabilities.items() if k.startswith(start_with)]\n",
    "        keys,vals = zip(*items_starting_with)\n",
    "        idx = vals.index(max(vals))\n",
    "        suggestion = keys[idx]\n",
    "        max_prob = vals[idx]\n",
    "\n",
    "    else:\n",
    "        suggestion = max(probabilities, key=probabilities.get)\n",
    "        max_prob = probabilities[suggestion]\n",
    "       \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9dd9a5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fee339914d8f54708e5562de741b3947",
     "grade": true,
     "grade_id": "Task9test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "\n",
    "\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "\n",
    "\n",
    "assert tmp_suggest2 == ('cat', 0.09090909090909091), \"Task 9 Test failed, Be sure to implement using startswith\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54883c01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d327e22628256780bfdebf9c458ac6cf",
     "grade": true,
     "grade_id": "cell-7854fc39a5cf1747",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ae766",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b96ee36924b1b1ac9693a9bf5fe5a289",
     "grade": false,
     "grade_id": "Add1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Additional Task 1 (2 points)\n",
    "\n",
    "By the suggest_a_word function, we have a simple autocomplete system, such that given previous tokens, it can predict the next word.\n",
    "Although we are predicting the word with the highest frequency, we can modify the function to suggest any random word.\n",
    "\n",
    "Write a function that suggests a random word and not the word with the highest probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a350cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def suggest_a_random_word(previous_tokens,n_minus_1_gram_counts , n_gram_counts , vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word randomly\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length >= n -1\n",
    "        n_minus_1_gram_counts: Dictionary of counts of (n-1)-grams\n",
    "        n_gram_counts: Dictionary of counts of (n)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of a random word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ## Get n which is how many words we are considering for the history\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    n_minus_one = n-1\n",
    "    ## From the incoming words that we want to predict the next word for, we can the n-1 words\n",
    "    n_minus_1_gram = previous_tokens[-n_minus_one:]\n",
    "    ## caluclate the probability of each word in the vocab being the next word\n",
    "    probabilities = estimate_probabilities(n_minus_1_gram, n_minus_1_gram_counts, n_gram_counts, vocabulary, k=k)\n",
    "\n",
    "    if(start_with == None):\n",
    "        # word,prob = random.choice(list(probabilities.items()))\n",
    "        word = np.random.choice(a = list(probabilities.keys()))\n",
    "        prob = probabilities[word]\n",
    "    else:\n",
    "        items_starting_with = [ (k,v) for k,v in probabilities.items() if k.startswith(start_with)]\n",
    "        word,prob = random.choice(items_starting_with)\n",
    "\n",
    "    return word,prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bee8fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The randomly suggested word is: is and the corresponding probability is : 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "\n",
    "\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = None\n",
    "tmp_suggest2 = suggest_a_random_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "\n",
    "print(f\"The randomly suggested word is: {tmp_suggest2[0]} and the corresponding probability is : {tmp_suggest2[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618a6fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "def366c07594a71ca8eddb2d07c9f031",
     "grade": false,
     "grade_id": "Add2",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Additional Task 2 (7 points)\n",
    "You can also put all the functions we have written together to autogenerate random texts of different lengths using different n-grams.\n",
    "You can also download the sample text of e-books here: https://www.gutenberg.org/ebooks/\n",
    "        \n",
    "Write a function that generates random texts of different lengths, using different n-grams.\n",
    "1. The function should take in a parameter that specifies the length of words to be generated \n",
    "1. The function should take in a parameter that specifies the n-gram to be used (i.e. the function should work using different ngrams).\n",
    "1. Use a body of raw text not less than 5000 words\n",
    "1. Preprocess the raw text using all the previous functions in the previous tasks.\n",
    "1. Write a function to calculate the perplexity metric using the function definition given below. \n",
    "\n",
    "![title](perplexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503dd30",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff64994",
   "metadata": {},
   "source": [
    "### Pre-processing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5191bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp.txt\", \"r\",encoding=\"UTF-8\") as txt_file:\n",
    " story = txt_file.readlines()\n",
    "\n",
    " data = [sentence_segmentation(s) for s in story if s!=\"\\n\"]\n",
    " tokenized_sentences = [tokenize_sentences(s) for s in data if \"Chapter\" not in s[0]]\n",
    "\n",
    "sentences = []\n",
    "for t in tokenized_sentences:\n",
    "    try:\n",
    "        ## Sentences will be a list of lists where each sublist is a sentence\n",
    "        sentences.extend(t)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20f6d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed !\n",
      "Corpus has 47044 words.\n"
     ]
    }
   ],
   "source": [
    "frequencies = count_tokens(sentences)\n",
    "total_number_of_words_in_corpus = sum(frequencies.values())\n",
    "\n",
    "assert (total_number_of_words_in_corpus >= 5000),\"Ensure Corpus has more than 5000 words\"\n",
    "print(\"Test Passed !\")\n",
    "print(f\"Corpus has {total_number_of_words_in_corpus} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "409dd54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_as_sentences = sentences\n",
    "\n",
    "## Getting the vocabulary of the corpus. A words is only part of the vocabulary if it occurs 3 or more times\n",
    "vocabulary = get_words_with_frequency(story_as_sentences,3)\n",
    "\n",
    "## Replacing words that are not in the vocabulary with <unk> tag\n",
    "story_as_sentences = replace_unknown_words(story_as_sentences,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd73b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_texts(length_of_words_to_be_generated,n,data,vocabulary):\n",
    "    \"\"\"\n",
    "    Generate a random sentence\n",
    "    \n",
    "    Args:\n",
    "        length_of_words_to_be_generated: integer denoting the number of words to be generated\n",
    "        n: integer denoting which n-gram to use\n",
    "        data: A list of lists where ach sublist contains a sentences of the corpus\n",
    "        vocabulary: The unique words of the corpus\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "       length_of_words_to_be_generated randomly generated sentence\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    ## Create the n_gram and n-1 gram\n",
    "    n_gram_counts = count_n_grams(data,n)\n",
    "    n_minus_one_gram_counts = count_n_grams(data,n-1)\n",
    "    i = 0\n",
    "    unkown= \"<unk>\"\n",
    "    stop = \"<e>\"\n",
    "    start = \"<s>\"\n",
    "    found = True\n",
    "    while(found):\n",
    "     ## randomly choose an n-1 gram\n",
    "        n_minus_one_gram = list(random.choice(list(n_minus_one_gram_counts.keys())))\n",
    "        # if(unkown in n_minus_one_gram or start in n_minus_one_gram or stop in n_minus_one_gram):\n",
    "        if(unkown in n_minus_one_gram):\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            found = False\n",
    "    ## Loop through and create length_of_words_to_be_generated number of words\n",
    "    while(i < length_of_words_to_be_generated):\n",
    "        ## Based on the n-1 gram, suggest a random word from the vocabulary\n",
    "        suggestion = suggest_a_random_word(n_minus_one_gram,n_minus_one_gram_counts,n_gram_counts,vocabulary)[0]\n",
    "        ## If the sugested word is any of the tags, make another suggestion\n",
    "        if(suggestion == unkown or suggestion == start or suggestion ==  stop):\n",
    "            continue\n",
    "        i += 1\n",
    "        ## store the generated sentence\n",
    "        sentences.append(suggestion)\n",
    "        ## Add the generated word to the n-1 gram\n",
    "        # Re-assign the n-1 gram\n",
    "        n_minus_one_gram.pop(0)\n",
    "        n_minus_one_gram.append(suggestion)\n",
    "    return \" \".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7a6fc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We asked for 4 randomly generated words using 6-gram.\n",
      "The sentence is: bridges kalidahs loving desire\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "length = 4\n",
    "random_text = generate_random_texts(length,n,story_as_sentences,vocabulary)\n",
    "print(f\"We asked for {length} randomly generated words using {n}-gram.\")\n",
    "print(f\"The sentence is: {random_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5b1d5a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87527653e2311c29c56a47880f56e79b",
     "grade": false,
     "grade_id": "cell-2724c78926b64b06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_minus_1_gram_counts, n_gram_counts, vocabulary_size, start_token, end_token, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_minus_1_gram_counts: Dictionary of counts of (n-1)-grams\n",
    "        n_gram_counts: Dictionary of counts of (n)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score to 4 decimal places\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    perplexity = 0.0000\n",
    "    n = len(list(n_minus_1_gram_counts.keys())[0])\n",
    "    sentence = [start_token]*n + sentence + [end_token]*n\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    total_probability = 1.0\n",
    "    for i in range(n,N):\n",
    "        n_minus_one_gram = sentence[i-n:i]\n",
    "        word = sentence[i]\n",
    "        probs = estimate_probability(word,n_minus_one_gram,n_minus_1_gram_counts,n_gram_counts,vocabulary_size,k)\n",
    "        total_probability *= 1/probs\n",
    "\n",
    "    perplexity = np.round(total_probability**(1/N),4)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46771d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the randomly generated sentence: bridges kalidahs loving desire, is: 479.1405\n",
      "The perplexity for a sentence from the book: When Dorothy stood in the doorway and looked around, is: 291.2747\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "unigram_counts = count_n_grams(story_as_sentences, 1)\n",
    "bigram_counts = count_n_grams(story_as_sentences, 2)\n",
    "\n",
    "text_from_book = \"When Dorothy stood in the doorway and looked around\"\n",
    "perplexity1 = calculate_perplexity(random_text.split(\" \"), unigram_counts, bigram_counts, len(vocabulary), start_token = \"<s>\", end_token = \"<e>\", k=1.0)\n",
    "perplexity2 = calculate_perplexity(text_from_book.split(\" \"), unigram_counts, bigram_counts, len(vocabulary), start_token = \"<s>\", end_token = \"<e>\", k=1.0)\n",
    "print(f\"The perplexity for the randomly generated sentence: {random_text}, is: {perplexity1}\")\n",
    "print(f\"The perplexity for a sentence from the book: {text_from_book}, is: {perplexity2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "124980ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for sentence 1 is: 2.804\n",
      "The perplexity for sentence 2 is: 3.0644\n",
      "The perplexity for the unseen sentence is: 4.8164\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity1 = calculate_perplexity(sentences[0], unigram_counts, bigram_counts, len(unique_words), start_token = \"<s>\", end_token = \"<e>\", k=1.0)\n",
    "perplexity2 = calculate_perplexity(sentences[1], unigram_counts, bigram_counts, len(unique_words), start_token = \"<s>\", end_token = \"<e>\", k=1.0)\n",
    "unseen = [\"i\",\"dog\",\"like\"]\n",
    "unseen_perplexity = calculate_perplexity(unseen, unigram_counts, bigram_counts, len(unique_words), start_token = \"<s>\", end_token = \"<e>\", k=1.0)\n",
    "print(f\"The perplexity for sentence 1 is: {perplexity1}\")\n",
    "print(f\"The perplexity for sentence 2 is: {perplexity2}\")\n",
    "print(f\"The perplexity for the unseen sentence is: {unseen_perplexity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
