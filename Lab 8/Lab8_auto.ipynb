{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd56cc92",
   "metadata": {},
   "source": [
    "### General Instructions\n",
    "\n",
    "Do not change the file name, method name or any variable name in your submission file. \n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and student number below.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Also, ensure that your notebook does not give errors before submitting. Ensure there is no 'Assertion Error', 'NotImplementedError' or  test(s) failed  feedback. \n",
    "\n",
    "NotImplementedError: this means there is a code cell/task you are yet to implement.\n",
    "\n",
    "AssertionError: this means your implementation is failing some tests.\n",
    "\n",
    "Note that your assignment will be checked with additional test cases after submission. Ensure you work with the instructions given.\n",
    "\n",
    "DO NOT EDIT ANY CELL/NOTEBOOK METADATA.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7493a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "STUDENT_NUMBER = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad709b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4e801-d531-447e-8e91-d712d0f38c58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04bf7fe14bb89516f00a5bdbdcf97324",
     "grade": false,
     "grade_id": "cell-d26cd8433aa16ea4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <center>  COMS4054A/COMS7066A </center>\n",
    "# <center> Natural Language Processing/Technology (NLP) 2022 </center>\n",
    "## <center> Lab Session 8 </center>\n",
    "### <center> 13th October, 2022 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917f2f-0cd9-40ab-b6c3-910338c69ddb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cfaa591a32d4c1f5c2321562d4f3efa",
     "grade": false,
     "grade_id": "cell-84529419bad9c0b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Sentiment classification with Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Implement a 2-layer neural network.\n",
    "- Use units with non-linear activation functions, (tanh and sigmoid)\n",
    "- Compute the cross entropy loss\n",
    "- Implement forward propagation\n",
    "- Implement backward propagation\n",
    "\n",
    "\n",
    "### Dataset\n",
    "We are using the NLTK Twitter dataset.\n",
    "\n",
    "Credit: Lab was adapted from a course by Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888e972",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "### Task Outline (Total 30 marks)\n",
    "\n",
    "Note: The marks (breakdown) are attached to the respective assertions.\n",
    "\n",
    "[Task 1](#task1) \n",
    "    \n",
    "[Task 2](#task2) \n",
    "    \n",
    "[Task 3](#task3)     \n",
    "\n",
    "[Task 4](#task4) \n",
    "\n",
    "[Task 5](#task5) \n",
    "\n",
    "[Task 6](#task6) \n",
    "\n",
    "[Task 7](#task7) \n",
    "\n",
    "[Task 8](#task8) \n",
    "\n",
    "[Task 9](#task9) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcce3d-20e6-41ce-a9b7-06addb712fad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da227a4fd856e305316ea471f1626f70",
     "grade": false,
     "grade_id": "cell-9f44aa5e8c47a405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "# Run this cell\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "import re \n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import  precision_recall_fscore_support, f1_score, precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4d397-f543-4da2-a35d-a06913e1a74b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a1a4149756d3a0d7e27e6ff6855bfd6",
     "grade": false,
     "grade_id": "cell-0f057bdcdb0af441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# downloads sample twitter dataset.\n",
    "nltk.download('twitter_samples')#Run this cell\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7a118-03e1-4e40-8c4e-a1995d57f782",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d875d19f66fda62171741cd2849a787e",
     "grade": false,
     "grade_id": "cell-a9ee2b027f03508b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell \n",
    "\n",
    "# Get the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5183393-d374-4aa7-9024-cc9dfedc0d83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cf83a882db1f6d782838acebb12a257",
     "grade": false,
     "grade_id": "cell-d2510f4c6e7709b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Preprocessing \n",
    "Read through, understand and run the following code cells to preprocess the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1664e-573d-4cc1-8bc8-1945e326e691",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "172d084e9569b56ed3e2148d609ae13e",
     "grade": false,
     "grade_id": "cell-34710bebe35c06b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to clean up tweets\n",
    "def clean_tweets(tweet):\n",
    "    \"\"\"\n",
    "    tweet (string)\n",
    "    return clean_tweet (string)\"\"\"\n",
    "    \n",
    "    \n",
    "    # remove  \"RT\"\n",
    "    clean_tweet = re.sub(r'^RT[\\s]+', '', tweet) #^ starts with RT, followed by one or more spaces\n",
    "\n",
    "    # remove hyperlinks\n",
    "    clean_tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', clean_tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    clean_tweet = re.sub(r'#', '', clean_tweet)\n",
    "    \n",
    "    return clean_tweet\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00b351-c60a-4801-86d4-791416ee9c32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6383324d939e8be729d711694360ec3",
     "grade": false,
     "grade_id": "cell-49923444822f56fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell \n",
    "\n",
    "def preprocess_pipeline(tweet):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    \"\"\" tweet(string)\n",
    "    return string\n",
    "    \"\"\"   \n",
    "    \n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    stemmer = PorterStemmer()    \n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = \"\"\n",
    "    tweets_clean = \" \".join([stemmer.stem(word) for word in tweet_tokens if (word not in stopwords_english)])\n",
    "    return tweets_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa0628-4e38-499f-926b-b77a2227610b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1219100d3c2cb9151116c8fade2b4521",
     "grade": false,
     "grade_id": "cell-bb8dc333c3ecea01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "all_positive_tweets_clean = []\n",
    "all_negative_tweets_clean = []\n",
    "\n",
    "for tw in all_positive_tweets:\n",
    "    all_positive_tweets_clean.append(preprocess_pipeline(clean_tweets(tw)))\n",
    "    \n",
    "for tv in all_negative_tweets:\n",
    "    all_negative_tweets_clean.append(preprocess_pipeline(clean_tweets(tv)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3a31b-3a24-472e-a8be-87267c3249a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ba5f587eea63ca179baef6527f3a5",
     "grade": false,
     "grade_id": "cell-61d3308b5fbd44cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "train_pos = all_positive_tweets_clean[:1000]\n",
    "train_neg = all_negative_tweets_clean[:1000]\n",
    "\n",
    "test_pos = all_positive_tweets_clean[1000:]\n",
    "test_neg = all_negative_tweets_clean[1000:]\n",
    "\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# putting 1 as label for the positve tweets, and 0 as label for negative tweets.\n",
    "\n",
    "train_y = np.concatenate((np.ones((len(train_pos),1),dtype=int),np.zeros((len(train_neg),1),dtype=int))).T\n",
    "test_y = np.concatenate((np.ones((len(test_pos),1),dtype=int), np.zeros((len(test_neg),1), dtype=int))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc57d1-cd5f-45e7-8998-7256226053b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3bc8d8e071e9864a4aaa4c0a262c0a9",
     "grade": false,
     "grade_id": "cell-7f72e7d42399d7cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Feature Extraction\n",
    "Run the next two code cells to extract the bag of words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bd3f3-f1a6-4099-8ceb-a5ee55cf40ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3180e5ba8becc4d41d0aa7681718dd5",
     "grade": false,
     "grade_id": "cell-ba42f408b83c0cdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "count_vec = CountVectorizer()\n",
    "train_features = np.array(count_vec.fit_transform(train_x).todense().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c80f8-8cc9-47e4-9ed3-efa8a3f0fa07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81729be971a412f54c8bcf25513710a7",
     "grade": false,
     "grade_id": "cell-3d822bc638e081c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "test_features = np.array(count_vec.transform(test_x).todense().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b965d-c05c-4d6d-9724-3c9512b4bcbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61e13466b435d6fe407952b2bd8b93b2",
     "grade": false,
     "grade_id": "cell-61ee7f2881384d71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#####  The goal is to stack the training examples vertically, hence the transpose.\n",
    "The number of features is going to be the number of nodes in the input layer.\n",
    "Let:\n",
    "- n_x -- the size of the input layer (same as number of features for each input)\n",
    "- n_h -- the size of the hidden layer\n",
    "- n_y -- the size of the output layer\n",
    "- m --- the size of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4112e4-751b-463d-8bab-5f5bf501ff42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5b8eee550dc7e39a4b01ba113208e90",
     "grade": false,
     "grade_id": "cell-012c1e33bed62c35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell \n",
    "\n",
    "shape_train_x = train_features.shape\n",
    "shape_train_y = train_y.shape\n",
    "m = train_features.shape[1]\n",
    "\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_train_x))\n",
    "print ('The shape of Y is: ' + str(shape_train_y))\n",
    "print ('Using %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131120ee-6b6a-47a1-943e-1a33ca01b7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b2365129166431d4ff00c56a1962327",
     "grade": false,
     "grade_id": "cell-cb9a783de2324ebf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Neural Network model\n",
    "\n",
    "**General overview**\n",
    "\n",
    "The main steps to build a Neural Network: (Andrew Ng; deeplearning.ai)\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "1. Initialize the model's parameters\n",
    "1. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)\n",
    "\n",
    "\n",
    "\n",
    "**The model**:\n",
    "\n",
    "\n",
    "<img src=\"nn.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "\n",
    "**Our task:**\n",
    "\n",
    "\n",
    "- To build a 2-layer network, with one hidden layer, with 4 hidden units/neurons.\n",
    "- We'll use `tanh` activation function at the hidden layer and the `sigmoid` activation function at the output layer.\n",
    "\n",
    "\n",
    "**Some notations to help you**\n",
    "\n",
    "\n",
    "superscript in round brackets indicates a training example\n",
    "\n",
    "superscript in square brackets indicates a layer number\n",
    "\n",
    "$z$ - weighted sum of features\n",
    "\n",
    "$a$ - output from a layer\n",
    "\n",
    "$x^{(i)}$ - the *i*th training example\n",
    "\n",
    "$z^{[1] (i)}$ -  the weighted sum of features for the *i*th example, from the first layer.\n",
    "\n",
    "$W^{[1]}$ - the weight matrix for layer 1\n",
    "\n",
    "$a^{[1] (i)}$ - the output from layer 1 on the ith training example\n",
    "\n",
    "$\\hat{y}^{(i)}$ - the final output (from the output layer) for the *i*th example.\n",
    "\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1129abc-6b8b-4e3c-a477-157f1ddbd0e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acd909fda94330ecc925a76e63173222",
     "grade": false,
     "grade_id": "cell-76758a3b8fd643b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task1'></a>\n",
    "\n",
    "[Back to top](#top) \n",
    "### Task 1: Define the neural network structure ####\n",
    "\n",
    "\n",
    "Define three variables:\n",
    "\n",
    "    - n_x: the size of the input layer\n",
    "    \n",
    "    - n_h: the size of the hidden layer (**set this to 4**) \n",
    "    \n",
    "    - n_y: the size of the output layer\n",
    "\n",
    "    - Use shapes of X (train_features) and Y (train_y) to find n_x and n_y. \n",
    "    - Hard code the hidden layer size to be 4.\n",
    "    - Implement this in function `layer_sizes`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ffbaf-8e2a-42e7-9407-08681a4cc4f7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24545d31b269adbe40edfbe6a5ee43e0",
     "grade": false,
     "grade_id": "T1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_sizes(train_features, train_y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    train_features -- input dataset of shape (input size/features, number of examples)\n",
    "    train_y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bc378-f152-4918-90d9-ff5cdaa444ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "875a6419f382c26750505a8d2d749cb4",
     "grade": false,
     "grade_id": "cell-340c459457b54b75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(train_features, train_y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510eaff5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af4159edaf8074c2f371abc44f7947d1",
     "grade": true,
     "grade_id": "cell-287aec16d298dc23",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "# 2 marks\n",
    "\n",
    "assert n_x == 5057\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b870e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca51f354bc4114beea36b0ab2edefd69",
     "grade": true,
     "grade_id": "cell-6d3ce7319f8e8fc4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "# 2 marks\n",
    "\n",
    "assert n_h == 4\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0c8d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b084eb88aad1f43d9f124a695acbf48",
     "grade": true,
     "grade_id": "cell-df7e11dbfb51f806",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "# 2 marks\n",
    "\n",
    "assert n_y == 1\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023326a-d16b-4526-93d0-6aee953c0eda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dceaf586684e232aef9b067d3ca0c34",
     "grade": false,
     "grade_id": "cell-13dea347b43b7011",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task2'></a>\n",
    "\n",
    "[Back to top](#top) \n",
    "### Task 2:  Initialize the model's parameters ####\n",
    "\n",
    "\n",
    "- Implement the function `initialize_parameters()`.\n",
    "\n",
    "- You will initialize the weights matrices with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- You will initialize the bias vectors as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56460150-fdee-43f3-a924-c0fa553a79db",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b062d07c9d30758ac9a285898f191ea",
     "grade": false,
     "grade_id": "task2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4442f95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2faddac23970b7ce3a842fb13fe759f6",
     "grade": false,
     "grade_id": "cell-6f15d08b38cae675",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d4dae-29da-4c2e-891c-65513ef10d55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb2014cb870b28f4038e17eeae588bf6",
     "grade": false,
     "grade_id": "cell-5e63cc904438e120",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# You should have implemented this function in Lab 5\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    sz = 1/(1+np.exp(-z))\n",
    "    return sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22cdfb-0bca-4205-82e3-82fc700cc611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd14e609282ad5b5486fbfcf14f2b9e1",
     "grade": false,
     "grade_id": "cell-a3324bf1dbe1e3d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='task3'></a>\n",
    "\n",
    "[Back to top](#top) \n",
    "### Task 3 - forward_propagation\n",
    "\n",
    "Implement `forward_propagation()` using the following equations:\n",
    "\n",
    "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$ \n",
    "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "\n",
    "- Use the function `sigmoid()`. It's inthe previous code cell.\n",
    "- Use the function `np.tanh()`. It's part of the numpy library.\n",
    "- Implement using these steps:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()` by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "- Values needed in the backpropagation are stored in \"cache\". The cache will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ae6b1-53b9-40b1-bc95-b203d5d51fbc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "567c1925b02c0e8c5ea2acf794b084cc",
     "grade": false,
     "grade_id": "task3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    # (≈ 4 lines of code)\n",
    "    # Z1 = ...\n",
    "    # A1 = ...\n",
    "    # Z2 = ...\n",
    "    # A2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e38993",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11ce6621677ad980b5385048766fab3c",
     "grade": true,
     "grade_id": "task3c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "# 3 marks\n",
    "\n",
    "_test_params = initialize_parameters(*layer_sizes(train_features, train_y))\n",
    "a2_ret, cache = forward_propagation(train_features, _test_params) # checks that it does not crash\n",
    "# checks the shape.\n",
    "N = train_features.shape[1]\n",
    "assert cache['A2'].shape == (1, N)\n",
    "assert cache['A1'].shape == (n_h, N)\n",
    "assert cache['Z2'].shape == (1, N)\n",
    "assert cache['Z1'].shape == (n_h, N)\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8ea1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e517c51507a9ed8cc2e8768d9cbe174",
     "grade": true,
     "grade_id": "task3d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "# 3 marks\n",
    "\n",
    "# Checks that the forward pass is correct.\n",
    "temp_features = np.ones((train_features.shape[0], 1))\n",
    "new_test_params = {}\n",
    "for k, v in _test_params.items():\n",
    "    new_test_params[k] = np.ones_like(v)\n",
    "_, ans_simple = forward_propagation(temp_features, new_test_params)\n",
    "ans_correct =  {'Z1': np.array([[5058.],\n",
    "         [5058.],\n",
    "         [5058.],\n",
    "         [5058.]]),\n",
    "  'A1': np.array([[1.],\n",
    "         [1.],\n",
    "         [1.],\n",
    "         [1.]]),\n",
    "  'Z2': np.array([[5.]]),\n",
    "  'A2': np.array([[0.99330715]])}\n",
    "for k, v in ans_simple.items():\n",
    "    assert np.allclose(v, ans_correct[k], atol=1e-3), f\"Item {k} is incorrect\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b7f83-c2ce-4b21-9392-24baa089e757",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e171c1e30d9ad5eae4c982dccebfcd",
     "grade": false,
     "grade_id": "cell-efac7815a201fe4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task4'></a>\n",
    "[Back to top](#top) \n",
    "### Task 4 - Compute the Cost\n",
    "\n",
    "Now that you've computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for all examples, you can compute the cost function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "\n",
    "\n",
    "Implement `compute_cost()` to compute the value of the cost $J$.\n",
    "\n",
    "\n",
    "**Notes**: \n",
    "\n",
    "- You can use either `np.multiply()` and then `np.sum()` or directly `np.dot()`).  \n",
    "- If you use `np.multiply` followed by `np.sum` the end result will be a type `float`, whereas if you use `np.dot`, the result will be a 2D numpy array.  \n",
    "- You can use `np.squeeze()` to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). \n",
    "- You can also cast the array as a type `float` using `float()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883b1e2-23ce-4ff7-975b-d296dc5e5e15",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d52d629b337e35ee01e6b7da9897b84",
     "grade": false,
     "grade_id": "task4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    # (≈ 2 lines of code)\n",
    "    # logprobs = ...\n",
    "    # cost = ...\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3272a93-4462-4c79-b373-8dd13c128db5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b3a36eb6574ae05c802d804ca5b3744",
     "grade": false,
     "grade_id": "cell-71faffc4605f9340",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task5'></a>\n",
    "\n",
    "[Back to top](#top) \n",
    "### Task 5 - Implement Backpropagation\n",
    "\n",
    "Using the cache computed during forward propagation, you can now implement backward propagation.\n",
    "\n",
    "\n",
    "Implement the function `backward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "Here are six equations. The equations on the right will help to implement a vectorized implementation.  \n",
    "\n",
    "*(Source: deeplearning.ai; Andrew Ng)*\n",
    "\n",
    "<img src=\"grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Backpropagation. Use the six equations on the right.</font></center></caption>\n",
    "\n",
    "<!--\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
    "\n",
    "- Note that $*$ denotes elementwise multiplication.\n",
    "- The notation you will use is common in deep learning coding:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "    \n",
    "!-->\n",
    "\n",
    "- Tips:\n",
    "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed6b19-174f-4ad6-bba7-7d83069741e4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a46cf1f4af5f01fd7a9c4057b680d1ee",
     "grade": false,
     "grade_id": "task5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # W1 = ...\n",
    "    # W2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # A1 = ...\n",
    "    # A2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    # dZ2 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # dZ1 = ...\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a082298",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "935d8f94f0296fa769cd2f21bfe31eb4",
     "grade": true,
     "grade_id": "task5a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Numerically calculate gradients. Note, this takes a while to run\n",
    "# (cost(w + h) - cost(w)) / h\n",
    "\n",
    "_test_params = initialize_parameters(*layer_sizes(train_features, train_y))\n",
    "\n",
    "small_x = train_features[:, :25]\n",
    "small_y = train_y[:, :25]\n",
    "    \n",
    "_, cache = forward_propagation(small_x, _test_params)\n",
    "ans = backward_propagation(_test_params, cache, small_x, small_y)\n",
    "    \n",
    "\n",
    "for k, v in ans.items():\n",
    "    print(\"Checking\", k)\n",
    "    h = 0.000001\n",
    "    weight_key = k[1:] # remove the 'd' prefix\n",
    "    actual_weight = _test_params[weight_key]\n",
    "    _new_params = copy.deepcopy(_test_params)\n",
    "    for i in range(actual_weight.shape[0]):\n",
    "        for j in range(actual_weight.shape[1]):\n",
    "            \n",
    "            _new_params[weight_key][i, j] += h\n",
    "            top = compute_cost(forward_propagation(small_x, _new_params)[0], small_y) - compute_cost(forward_propagation(small_x, _test_params)[0], small_y)\n",
    "            assert np.allclose(top / h, v[i, j])\n",
    "            _new_params[weight_key][i, j] -= h\n",
    "    print(k, \"Passed\")\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e45a1-08f0-4ba5-9734-2e62920dcd0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5c1071b14d38f293828eb28567a748c",
     "grade": false,
     "grade_id": "tak6q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task6'></a>\n",
    "[Back to top](#top) \n",
    "\n",
    "### Task 6 -  Update Parameters \n",
    "\n",
    "\n",
    "Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
    "\n",
    "**General gradient descent rule**: $\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "<img src=\"sgd.gif\" style=\"width:400;height:400;\"> <img src=\"sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</font></center></caption>\n",
    "\n",
    "**Hint**\n",
    "\n",
    "- Use `copy.deepcopy(...)` when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cd7eb-3c18-4e9b-a59d-4d02ae490446",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f54c0f17676ee41ee3c8bfd00f3c777e",
     "grade": false,
     "grade_id": "task6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24a907",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1252fcd6d5be38079ca4a9a96bcf6a21",
     "grade": true,
     "grade_id": "task6a",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Checks simple update\n",
    "_test_params = {\n",
    "    \"W1\": np.zeros((4, 1)),\n",
    "    \"b1\": np.zeros((4, 5057)),\n",
    "    \"W2\": np.zeros((1, 4)),\n",
    "    \"b2\": np.zeros((1, 1))\n",
    "}\n",
    "\n",
    "_test_grads = {\n",
    "    \"dW1\": np.ones((4, 1)),\n",
    "    \"db1\": np.ones((4, 5057)),\n",
    "    \"dW2\": np.ones((1, 4)),\n",
    "    \"db2\": np.ones((1, 1))\n",
    "}\n",
    "ans = update_parameters(_test_params, _test_grads, learning_rate=0.1)\n",
    "for k, v in ans.items():\n",
    "    assert np.allclose(v, -0.1)\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415813e1-64e9-4edf-acc4-4fe1d71bec4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d695a3933e11a3346b0ed5f3b9d5fbd",
     "grade": false,
     "grade_id": "cell-6fc059ddfaa420bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task7'></a>\n",
    "[Back to top](#top) \n",
    "### Task 7 - Integration\n",
    "\n",
    "Integrate your functions in `nn_model()` \n",
    "\n",
    "Build your neural network model in `nn_model()`.\n",
    "\n",
    "**Instructions**: The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eea016-15ea-4bbf-8e61-eeb9d5671516",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef4b9670af5246aa7d70101ad51b8aa9",
     "grade": false,
     "grade_id": "task7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    loss_start = None\n",
    "    loss_end = None\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        #(≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        # A2, cache = ...\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n",
    "        # cost = ...\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        # grads = ...\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        # parameters = ...\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        loss_start = cost if loss_start is None else loss_start\n",
    "        loss_end = cost\n",
    "    assert loss_start >= loss_end, f\"Test failed! The loss at the start {loss_start} must be larger than the loss at the end of training {loss_end}\"\n",
    "    print(\"Test Passed\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b058fb-c002-49f0-b129-ff36bfd9a8e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5e6e39094b263d3c2e162503ab4713d",
     "grade": false,
     "grade_id": "cell-37e7d364a555ef30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task8'></a>\n",
    "[Back to top](#top) \n",
    "### Task 8 - Test the Model (Peedict)\n",
    "\n",
    "\n",
    "Predict with your model by building `predict()`.\n",
    "Use forward propagation to predict results.\n",
    "\n",
    "Predictions = $ \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise (<= 0.5)}\n",
    "    \\end{cases}$  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6160bf-c0b7-4507-8330-8e8c721d6106",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f30e74a643fff95b88cd509166ff5e6c",
     "grade": false,
     "grade_id": "task8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model \n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    #(≈ 2 lines of code)\n",
    "    # A2, cache = ...\n",
    "    # predictions = ...\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57968251-60db-40ae-b613-59de3916214d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd17be72dce6b87cdb02135838b22b9c",
     "grade": false,
     "grade_id": "cell-1e975e814620dd0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "parameters =  nn_model(train_features, train_y, n_h, num_iterations = 1000, print_cost=True)\n",
    "\n",
    "predictions = predict(parameters, test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0fe7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef4a4610b1df4f53162cd5483a5721cf",
     "grade": false,
     "grade_id": "cell-6f1e37b72268d3ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task9'></a>\n",
    "\n",
    "[Back to top](#top) \n",
    "### Task 9 - Evaluation\n",
    "Implement the function evaluate to calculate and return the accuracy, f1 score, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523d3f4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b47ea03068ad80de852c37405bab91f7",
     "grade": false,
     "grade_id": "task9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(predictions,test_y):\n",
    "    predictions = np.reshape(predictions, newshape=(-1))\n",
    "    test_y = np.reshape(test_y, newshape=(-1))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0502f01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c501345f98fe293c0e0397e01e79488",
     "grade": true,
     "grade_id": "cell-23b5b7393bd3d4be",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_test_preds   = np.array([1, 0, 1, 0, 1, 1, 1, 1])\n",
    "_test_correct = np.array([0, 1, 1, 0, 1, 1, 1, 0])\n",
    "assert np.allclose(np.round(evaluate(_test_preds, _test_correct), 3), (0.625, 0.727, 0.667, 0.8))\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ef432-5d86-4d20-99d5-2dfba8b17f11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "659a22738871eb2b5f04b28a372cc8d7",
     "grade": true,
     "grade_id": "task9a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate(predictions,test_y)[0]\n",
    "assert accuracy >= 0.60\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818a9de-c754-4b94-9651-cdca83a0fb26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcc4d7ab74962bce1cfc25a52904e621",
     "grade": true,
     "grade_id": "task9b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f1 = evaluate(predictions,test_y)[1]\n",
    "assert f1 >= 0.60\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947de78b-c18b-43a5-8662-97e07b5b8aaf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcf35fe27d481495b2abcbe2521b0da3",
     "grade": true,
     "grade_id": "task9c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "precision = evaluate(predictions,test_y)[2]\n",
    "assert precision >= 0.60\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcf988",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af9a30abcfb266c1c181a73d64688eb9",
     "grade": true,
     "grade_id": "task9d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "recall = evaluate(predictions,test_y)[3]\n",
    "assert recall >= 0.60\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842de925-5ef7-4d19-923d-748ccf23f30d",
   "metadata": {},
   "source": [
    "##### Note that running the neural network with the entire dataset (10000 tweets) might take some time, depending on your computation power.\n",
    "Try increasing the portion of the dataset used, with a larger number of iterations and you will get better results.\n",
    "You can run your code on Google colab, if you are struggling with compute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
