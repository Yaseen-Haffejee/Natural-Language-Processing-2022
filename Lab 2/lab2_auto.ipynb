{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f0e4cf",
   "metadata": {},
   "source": [
    "### General Instructions\n",
    "\n",
    "Do not change the file name, method name or any variable name in your submission file. \n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and student number below.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Also, ensure that your notebook does not give errors before submitting. Ensure there is no 'Assertion Error', 'NotImplementedError' or  test(s) failed  feedback. \n",
    "\n",
    "NotImplementedError: this means there is a code cell/task you are yet to implement.\n",
    "\n",
    "AssertionError: this means your implementation is failing some tests.\n",
    "\n",
    "Note that your assignment will be checked with additional test cases after submission. Ensure you work with the instructions given\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a846d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Yaseen Haffejee\"\n",
    "STUDENT_NUMBER = \"1827555\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc560f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bfcea-6fbb-4f3b-a3e7-d055dfe920da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad5def432dd3a4eac8aa9b03ad382891",
     "grade": false,
     "grade_id": "cell-f087bbb7ff5e5577",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <center>  COMS4054A/COMS7066A </center>\n",
    "# <center> Natural Language Processing/Technology (NLP) 2022 </center>\n",
    "## <center> Lab Session 2 </center>\n",
    "### <center> 4 August, 2022 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358a9fd-9a45-4727-9c53-7f8d6cce3cf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51859f49adcf625d3f3ee9255469de87",
     "grade": false,
     "grade_id": "cell-be0d4250347a8106",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Text Normalization and Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137f8c2-a313-45fa-8578-d285f8f6ce41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64a0ece3b0adf67efe5fa9b117c716aa",
     "grade": false,
     "grade_id": "cell-3052bf40f6ffa812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Objectives\n",
    "\n",
    "The goal of this session is to have a practical engagement on the topics discussed in Lecture 2.\n",
    "\n",
    "- Perform different text normalization tasks using raw Python code as well as the NLTK library.\n",
    "   - Removal of punctuations\n",
    "   - Tokenization\n",
    "   - Stemming\n",
    "   - Lemmatization etc.\n",
    "- Implement the minimum edit distance algorithm.\n",
    "- Build a spell checker using the minimum edit distance algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290ae7a-e040-48ef-98ea-ee26d74cd7e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16643d23eb4b850829f1ad7288a06a03",
     "grade": false,
     "grade_id": "cell-3ebed0687b8362c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Outline\n",
    "- Lesson 1 - Text Normalization\n",
    "- Lesson 2 - Minimum Edit Distance and Spell Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f319394-95c0-4454-9df0-dc69f4874855",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40b6c7a64dd1dbf41ac920d8de7d363c",
     "grade": false,
     "grade_id": "cell-b1df79918f527ab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "- Think about it and implement your method/approach before looking at the hints.\n",
    "- The hints are just one way to look at the tasks.\n",
    "- You do not have to follow the approach given in the hint.\n",
    "- Download the required files (testing.txt, correct_words.txt, incorrect_words.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab39b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3436c0bba26dc0ce9775df3cb6b78e0c",
     "grade": false,
     "grade_id": "cell-6417c72361e81be6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Marks Allocation (Total - 30 points)\n",
    "Tasks 1, 2 and 3: 10 points\n",
    "\n",
    "Task 4: 6 points\n",
    "\n",
    "Task 5: 5 points\n",
    "\n",
    "Task 7: 9 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc7d23-f15e-474f-9923-d6984e8a6e85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8df705a859ea3b190d696c1fd7ea4232",
     "grade": false,
     "grade_id": "cell-0cc96165556fb151",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Lesson 1 - Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db28399-5f23-491f-8b37-eb805611d298",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9d37f6073833aca5f18fe20256c0e71",
     "grade": false,
     "grade_id": "cell-37bd2ff82562feb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Question 1.1\n",
    "\n",
    "Write a simple data preprocessing pipeline in raw Python code. Do not use any libraries.\n",
    "Given a piece of text in English language, your program should:\n",
    "- Remove punctuations (Use string.punctuation in python for the list of all punctuations)\n",
    "- Tokenize the string of text (split by spaces)\n",
    "- Perform casefolding (turn all text to lowercase)\n",
    "- Return the cleaned tokens as a list\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625207d9-36ec-422e-9ff6-bba810a99305",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80c88603ea278ea543dabad218b71ba1",
     "grade": false,
     "grade_id": "cell-38c3b1c8bad870b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You! I came here yesterday, but I didn't see you. So where were you? She is eighty-seven years today. I am learning NLP, I love love NLP\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('testing.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n',\" \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcc5d7-91f6-4440-93fa-449335acb01b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14e8ce8b5aff345f52753dd3b52467ba",
     "grade": false,
     "grade_id": "cell-4003079867f1ed78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 1: Write code to remove punctuation\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "You can iterate through the text character by character, then join together afterwards    \n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "![title](img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73be5570-96ca-4fe0-8db9-e587e4133118",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f051b0022015f85ecd03278404c93874",
     "grade": false,
     "grade_id": "Task1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "test_string = data\n",
    "\n",
    "# this function should return your text with punctuations removed\n",
    "\n",
    "def task1(test_string):\n",
    "    # YOUR CODE HERE\n",
    "    punct_removed = test_string.translate(test_string.maketrans(\"\",\"\",string.punctuation))\n",
    "    return punct_removed\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "punct_removed = task1(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a0fcec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53981121630c1a618fc130b2fa28d00f",
     "grade": true,
     "grade_id": "Task1_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert task1(test_string) == 'You I came here yesterday but I didnt see you So where were you She is eightyseven years today I am learning NLP I love love NLP',\"Task 1 test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1803a1ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ae4e022b80085569295e4c0e0fa14db",
     "grade": true,
     "grade_id": "Task1_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe2cf6-150f-49dc-94c8-5e7f552d7d78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36945498f3fbe6ab43747d6bf234b7db",
     "grade": false,
     "grade_id": "cell-08cb8705f14d1c17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 2: Write code to remove the spaces\n",
    "\n",
    "Write it as a function. \n",
    "\n",
    "Input = output from task 1\n",
    "\n",
    "Output = test string without spaces, returned as a list\n",
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "string has a split() method.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "![title](img2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151ef97f-4398-4ec7-957f-e2c9765953f0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4912cec717fa245944aa6454a7b7beee",
     "grade": false,
     "grade_id": "Task2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'I', 'came', 'here', 'yesterday', 'but', 'I', 'didnt', 'see', 'you', 'So', 'where', 'were', 'you', 'She', 'is', 'eightyseven', 'years', 'today', 'I', 'am', 'learning', 'NLP', 'I', 'love', 'love', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "def task2(punct_removed):\n",
    "    # YOUR CODE HERE\n",
    "    spaces_removed = punct_removed.split(\" \")\n",
    "    return spaces_removed\n",
    "    raise NotImplementedError()\n",
    "spaces_removed = task2(punct_removed)\n",
    "print(spaces_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34364ff7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "504bba47d301f90284c3827f417f4818",
     "grade": true,
     "grade_id": "Task2_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert task2(task1(test_string)) == ['You', 'I', 'came', 'here', 'yesterday', 'but', 'I', 'didnt', 'see', 'you', 'So', 'where', 'were', 'you', 'She', 'is', 'eightyseven', 'years', 'today', 'I', 'am', 'learning', 'NLP', 'I', 'love', 'love', 'NLP'] ,\"Task 2 test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac27f8-0b99-4224-a63e-d41ed1db418d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7971a7372d67ddf11b4a71345c494a77",
     "grade": false,
     "grade_id": "cell-05eaae0f4e2dd786",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 3: Write code to perform case folding\n",
    "\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "![title](img3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f368c642-4a96-4ee5-a449-2a20ff59a238",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed86035f54325d34fcca52a96793c7be",
     "grade": false,
     "grade_id": "Task3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def task3(spaces_removed):\n",
    "    to_lower_case = []\n",
    "\n",
    "    for word in spaces_removed:\n",
    "        to_lower_case.append(word.lower())#type code here\n",
    "\n",
    "    return to_lower_case # this variable should contain your output\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a24cacd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79537c59c5b627f9619ff02243831e36",
     "grade": true,
     "grade_id": "Task3_1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert task3(task2(task1(test_string))) == ['you', 'i', 'came', 'here', 'yesterday', 'but', 'i', 'didnt', 'see', 'you', 'so', 'where', 'were', 'you', 'she', 'is', 'eightyseven', 'years', 'today', 'i', 'am', 'learning', 'nlp', 'i', 'love', 'love', 'nlp'],\"Task 3 test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15248372",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0dc7ee43cf6df2e4928e2816f3d8d27",
     "grade": true,
     "grade_id": "Task1_2_3",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787e465-1529-4c9a-9389-f6f71d473b23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf721a2aa1929129efa2ebfe63c586c8",
     "grade": false,
     "grade_id": "cell-be62b83590768a97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Question 1.2\n",
    "- Use the NLTK to perform the same task\n",
    "- Ensure you get exactly the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81945e34-9d9f-44aa-9bd2-c91902ce7cd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab16d4595979d4a2fc08d12d795973ce",
     "grade": false,
     "grade_id": "cell-6f70c44dffc8eb53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', '!', 'I', 'came', 'here', 'yesterday', ',', 'but', 'I', 'did', \"n't\", 'see', 'you.', 'So', 'where', 'were', 'you', '?', 'She', 'is', 'eighty-seven', 'years', 'today.', 'I', 'am', 'learning', 'NLP', ',', 'I', 'love', 'love', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "token_punkt = word_tokenize(test_string, preserve_line=True)\n",
    "print(token_punkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0758f-e509-4d5f-8018-cf6fc0bf6b81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ca803f35902b05a830c2d585d98961",
     "grade": false,
     "grade_id": "cell-98379443545b28cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How does this work? \n",
    "It separated the punctuation marks as tokens.\n",
    "How do we remove the punctuation marks? \n",
    "There are different ways around this with different implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7313dfec-c01d-40bd-bafa-8ae03fa55d7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62f1d3784510cbcf571c5415327e7d87",
     "grade": false,
     "grade_id": "cell-e44f2f09ced503ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'I', 'came', 'here', 'yesterday', 'but', 'I', 'did', 'see', 'So', 'where', 'were', 'you', 'She', 'is', 'years', 'I', 'am', 'learning', 'NLP', 'I', 'love', 'love', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "def remove_punct(token):\n",
    "    return [word for word in token if word.isalpha()]\n",
    "\n",
    "token_punkt_ = remove_punct(token_punkt)\n",
    "print(token_punkt_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bf8f4-7780-4ffd-8c52-ed5cb4248b0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9560b227ed310ceeb0d4f932390f18c5",
     "grade": false,
     "grade_id": "cell-b24862f85a969a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Something is missing. Is this the output we want?\n",
    "\n",
    "It is good to know that differen tokenizers behave differently. Always test and use the one that suits your purpose. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64cfc66f-8080-4bc1-aa24-1e9ff33c1364",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69ade8642fe59159438a349dc73e9136",
     "grade": false,
     "grade_id": "cell-74825c76886a154c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'I',\n",
       " 'came',\n",
       " 'here',\n",
       " 'yesterday',\n",
       " 'but',\n",
       " 'I',\n",
       " 'didn',\n",
       " 't',\n",
       " 'see',\n",
       " 'you',\n",
       " 'So',\n",
       " 'where',\n",
       " 'were',\n",
       " 'you',\n",
       " 'She',\n",
       " 'is',\n",
       " 'eighty',\n",
       " 'seven',\n",
       " 'years',\n",
       " 'today',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " 'I',\n",
       " 'love',\n",
       " 'love',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Different tokenizers behave differently\n",
    "# see http://text-processing.com/demo/tokenize/\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948866d7-0e88-450c-862f-347277bb6839",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "203b740997521d14b5f338c6270fcce8",
     "grade": false,
     "grade_id": "cell-b44de336af0671cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 4: Modify the output.\n",
    "\n",
    "Let's assume we want exactly the same behaviour as was implemented in Question 1.\n",
    "\n",
    "Write a function that takes in the output from the TweekTokenizer, 'tweet_tokenizer_tokens', processes it (remove punctuations etc.) and gives exactly the same output as when we tokenized manually. In other words, your output should be the same as 'to_lower_case' from Question 1.1.\n",
    "\n",
    "Expected Output:\n",
    "![title](img4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "519b5c9f-bda6-45a4-9fdc-1ad05b7cdf7e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "088bdcb66ba86618d190fdda80caf6eb",
     "grade": false,
     "grade_id": "Task4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', '!', 'I', 'came', 'here', 'yesterday', ',', 'but', 'I', \"didn't\", 'see', 'you', '.', 'So', 'where', 'were', 'you', '?', 'She', 'is', 'eighty-seven', 'years', 'today', '.', 'I', 'am', 'learning', 'NLP', ',', 'I', 'love', 'love', 'NLP']\n",
      "['you', 'i', 'came', 'here', 'yesterday', 'but', 'i', 'didnt', 'see', 'you', 'so', 'where', 'were', 'you', 'she', 'is', 'eightyseven', 'years', 'today', 'i', 'am', 'learning', 'nlp', 'i', 'love', 'love', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokenizer_tokens = tweet_tokenizer.tokenize(test_string)\n",
    "print(tweet_tokenizer_tokens)\n",
    "\n",
    "def remove_punct2(token):\n",
    "    list_words = []\n",
    "    # YOUR CODE HERE\n",
    "    for word in token:\n",
    "        ## Use regex to remove any special characters which denotes the punctuations\n",
    "        w = re.sub(\"\\W\",\"\",word)\n",
    "        if(w != \"\"):\n",
    "            list_words.append(w.lower())\n",
    "    return list_words\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tokens_twitter = remove_punct2(tweet_tokenizer_tokens)\n",
    "print(tokens_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13309e2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67ae8e0d257489c025c1f0606f6a0f8d",
     "grade": true,
     "grade_id": "Task4_hidden",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f38b8e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fba1c60c8b0686adb2f9c794308d2f3c",
     "grade": true,
     "grade_id": "Task4_1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert remove_punct2(tweet_tokenizer_tokens) == ['you', 'i', 'came', 'here', 'yesterday', 'but', 'i', 'didnt', 'see', 'you', 'so', 'where', 'were', 'you', 'she', 'is', 'eightyseven', 'years', 'today', 'i', 'am', 'learning', 'nlp', 'i', 'love', 'love', 'nlp'],\"Task 4 test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0ccf1d4-ae5b-48b5-a224-2e8eb4ee9644",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b90f4c2dbd285f183e3ef15b61786c",
     "grade": false,
     "grade_id": "cell-862311d49d395a01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#test both strings and report if they are the same or not.\n",
    "print(tokens_twitter == to_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc2cf9-a693-4645-8751-2608da4ee8bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9888aaa70f61e5a90d7a5875f5f74704",
     "grade": false,
     "grade_id": "cell-9453afcebce9543d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Question 1.3 - Stemming and Lemmatization\n",
    "Let's use the PorterStemmer and SnowballStemmer on a sample tokenized text.\n",
    "You can also try them out on your final output from the previous question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f8cc215-b395-41f8-a922-24072c32ecf5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb9c94435aada16eac05df162e219ad7",
     "grade": false,
     "grade_id": "cell-73beaa2185835f2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'segmenting', 'running', 'text', 'into', 'sentences', 'and', 'words', '.', 'In', 'essence', ',', \"it's\", 'the', 'task', 'of', 'cutting', 'a', 'text', 'into', 'pieces', 'called', 'tokens', '.']\n",
      "['tokenization', 'is', 'the', 'process', 'of', 'segmenting', 'running', 'text', 'into', 'sentences', 'and', 'words', 'in', 'essence', 'its', 'the', 'task', 'of', 'cutting', 'a', 'text', 'into', 'pieces', 'called', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "test_string_2 = \"Tokenization is the process of segmenting running text into sentences and words. In essence, it's the task of cutting a text into pieces called tokens.\"\n",
    "test_string_2_tokens = tweet_tokenizer.tokenize(test_string_2)\n",
    "print(test_string_2_tokens)\n",
    "\n",
    "test_string_2_tokens = remove_punct2(test_string_2_tokens)\n",
    "print(test_string_2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ba9b4d0-4090-4f8a-a502-0229ed92899e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a5b3bf653332f22efa53d002c255921",
     "grade": false,
     "grade_id": "cell-fd5e973ce72efe80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'is', 'the', 'process', 'of', 'segment', 'run', 'text', 'into', 'sentenc', 'and', 'word', 'in', 'essenc', 'it', 'the', 'task', 'of', 'cut', 'a', 'text', 'into', 'piec', 'call', 'token']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps_stem_text = [ps.stem(words_sent) for words_sent in test_string_2_tokens]\n",
    "print(ps_stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b70b66-e5f6-497a-9f5c-fda555447543",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da45d7b8390e8c4b224cb5d3532028f4",
     "grade": false,
     "grade_id": "cell-c152388a5a63aafa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There's also the Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66654684-fd29-42b3-b320-56aea727f2f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee5ad7e92a35ada00f23b8440438d9d7",
     "grade": false,
     "grade_id": "cell-add6c6ed73e47517",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'is', 'the', 'process', 'of', 'segment', 'run', 'text', 'into', 'sentenc', 'and', 'word', 'in', 'essenc', 'it', 'the', 'task', 'of', 'cut', 'a', 'text', 'into', 'piec', 'call', 'token']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "sb_stem_sent = [sb.stem(words_sent) for words_sent in test_string_2_tokens]\n",
    "print(sb_stem_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8835a",
   "metadata": {},
   "source": [
    "### Notes for myself\n",
    "Stemming allows us to reduce words into their root form. This ensures we do not have duplicates in our data. E.g, connects, connecting have the same root word of connect.<br>\n",
    "Sometimes stemming may produce us with words/stems that are not actually part of the language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6478985-ff91-474a-92b7-16e27920e3a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9988c990e7bfbc114391a56f8297505",
     "grade": false,
     "grade_id": "cell-f89c7f99e6126597",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Question 1.4 - Lemmatization\n",
    "\n",
    "Let's use the WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f49b8d7f-cafa-4e23-ade9-533931a6218b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3de7e3ff6bdfab656a253e2edc49d898",
     "grade": false,
     "grade_id": "cell-63da2344a5ce06cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download (\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ad1c294-2d19-4b0f-a470-d07312434cc0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f80e5d8cbd50636c26193365af8562d5",
     "grade": false,
     "grade_id": "cell-f847e3ec7650319c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization', 'is', 'the', 'process', 'of', 'segmenting', 'running', 'text', 'into', 'sentence', 'and', 'word', 'in', 'essence', 'it', 'the', 'task', 'of', 'cutting', 'a', 'text', 'into', 'piece', 'called', 'token']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_sent = [lemmatizer.lemmatize(words_sent) for words_sent in test_string_2_tokens]\n",
    "print(lem_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f63bb-fdfd-48e0-ba1e-7fb039409c88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeee82cfa4a84e15578fa888f2b8436f",
     "grade": false,
     "grade_id": "cell-8697af341b81ae6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Just some notes...\n",
    "\n",
    "Did you notice the differences between stemming and lemmatization. \n",
    "Try out other words or sentences.\n",
    "\n",
    "- Stemming simply chops off the end of the word, based on some heuristics\n",
    "- The stem word does not have to be a real word in the given language. E.g. datum can be reduced to datu (which is not in the English dictionary).\n",
    "- Lemmatization reduces to the base word, it ensures the root word belongs to the given language\n",
    "- Lemmatization hurts performance though\n",
    "\n",
    "Is text normalization always required or beneficial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981acc8",
   "metadata": {},
   "source": [
    "- Yes, text normalisation is always required since it allows us to reduce the randomness in our data.By reducing the randomness, we essentially have a standard for the text which the computer will process. Having this standard will allow us to have more efficient models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c0c9f-79f0-462a-b7f6-e26597927328",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a7dd4e374a263a3730b9a6f18ae25ba",
     "grade": false,
     "grade_id": "cell-f76b33a37feb6ce2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Question 1.5 - Removing Stop words\n",
    "\n",
    "Sometimes, you might also need to remove stop words (common words) from the text.\n",
    "\n",
    "The NLTK has a set of stopwords, let's download that and remove it from a sample text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7faec1e-1dbd-403a-902c-8705d0df0628",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34e6ca59d1c506d1d098c92551664fe6",
     "grade": false,
     "grade_id": "cell-13711dbdfec1b794",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # Natural Language tool kit \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# You have to download stopwords Package to execute this command\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# check out the stop words in English\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e526a-afac-4203-bc31-fab92b632358",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "414621a7e22a514db2fbf59e97bbf473",
     "grade": false,
     "grade_id": "cell-decdcf79b00a40ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 5\n",
    "Remove the stops words in the lemmatized sentence 'lem_sent'.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "![title](img5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "207252bb-313e-46aa-95ca-59ae47d56c6a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2d0c1557780c027b0ba375826c2e723",
     "grade": false,
     "grade_id": "Task5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenization',\n",
       " 'process',\n",
       " 'segmenting',\n",
       " 'running',\n",
       " 'text',\n",
       " 'sentence',\n",
       " 'word',\n",
       " 'essence',\n",
       " 'task',\n",
       " 'cutting',\n",
       " 'text',\n",
       " 'piece',\n",
       " 'called',\n",
       " 'token']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_sent_remove_stopwords = [word for word in lem_sent if word not in stopwords.words('english')]\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "lem_sent_remove_stopwords # Only important (not so common) words are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f32dbd8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09d4b0a7c1d2103f16ab29f7b3ab22b0",
     "grade": true,
     "grade_id": "Task5_1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert lem_sent_remove_stopwords == ['tokenization',\n",
    " 'process',\n",
    " 'segmenting',\n",
    " 'running',\n",
    " 'text',\n",
    " 'sentence',\n",
    " 'word',\n",
    " 'essence',\n",
    " 'task',\n",
    " 'cutting',\n",
    " 'text',\n",
    " 'piece',\n",
    " 'called',\n",
    " 'token'], \"Task 5 test(s) failed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee80ddc0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e0666b5deb748b9a58fd5cb5cdfac6b",
     "grade": true,
     "grade_id": "cell-18f28aab13017ff9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda66af7-d536-467a-86d4-0c74be48f12c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac2ec0184b91f0c7e36cbc1be0c31158",
     "grade": false,
     "grade_id": "cell-dcc6a7c35727e5e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Lessson 2 - Minimum Edit distance\n",
    "\n",
    "#### Question 2.1\n",
    "Implement the edit distance algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45aa1fe-4350-4ac4-9f8f-1f13c0a2a6d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "505394a94ae9ce44c4206018710e2439",
     "grade": false,
     "grade_id": "cell-0b54420b23d25bf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 6\n",
    "\n",
    "Read through for understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdd29e18-2f50-47a2-91a3-d9adc9bb1ab6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bffa97fe95d5fbe5e9156aef4113ca53",
     "grade": false,
     "grade_id": "cell-6f047a9174a2fd11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(0,m+1): # Replace None with the proper range\n",
    "        D[row,0] = row\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(0,n+1): # Replace None with the proper range\n",
    "        D[0,col] = col\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1):\n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]: # Replace None with a proper comparison\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min([D[row-1, col-1]+r_cost, D[row,col-1]+ins_cost, D[row-1, col]+del_cost])\n",
    "            \n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[row,col]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb71974-88b1-482b-9316-f5e2336da1fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd510d9cdc3641ea6f44a8928d167f81",
     "grade": false,
     "grade_id": "cell-c7c7702a9d25d67f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What is the minimum distance between 'dare' and 'dear'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81a6ce98-3102-4416-af14-ef31268bd524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(min_edit_distance(\"dare\", \"dear\", ins_cost = 1, del_cost = 1, rep_cost = 2)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943211a-994c-425f-a434-4f8be896a9d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94157fd098bf69864f0d893522a611ea",
     "grade": false,
     "grade_id": "cell-fe0520284527a05a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 7 - Build a Spell checker\n",
    "- Download the files 'correct_words.txt' and 'incorrect_words.txt'\n",
    "- As the name implies, 'correct_words' contain a list of correctly spelled words, while 'incorrect_words' is a list of incorrectly spelled words.\n",
    "- Extract your vocabulary from 'correct_words.txt'. All words in the file are to be in the vocabulary.\n",
    "- For each incorrect word in 'correct_words.txt', the task is to suggest a correct spelling from the list of correct words in 'correct_words.txt'.\n",
    "   - Complete the function 'correct_spelling'. \n",
    "     - It takes in an incorrect word and the list of correct words (vocabulary)\n",
    "     - It returns a new_word suggestion that is closest to the incorrect word from the vocabulary\n",
    "        - Calculate the minimum edit distance between the incorrect word and all correct words, to find the closest word. Based on your calculation, suggest the possible intended word.\n",
    "        - Use the min_edit_distance function given\n",
    "     \n",
    "A sample of the expected output is given:\n",
    "\n",
    "![title](img6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6dff320-2004-4245-b655-094dbc7f41ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adbc8b4061e5ca35a3613bcbb2422f66",
     "grade": false,
     "grade_id": "Task7_a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open('correct_words.txt', 'r') as file:\n",
    "    #data = file.read().replace('\\n',\" \")\n",
    "    data = file.readlines()\n",
    "    vocabulary = [correct.replace(\"\\n\",\"\") for correct in data]# complete the code\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "with open('incorrect_words.txt', 'r') as file:\n",
    "    #data = file.read().replace('\\n',\" \")\n",
    "    data = file.readlines()\n",
    "    incorrect_words = [a.replace('\\n',\"\") for a in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd90e2af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2770efb5263cb6f09f290552303a6f6",
     "grade": true,
     "grade_id": "Task7_a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7ce30fb-3d00-42c8-9a8a-6e69c4388d8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ee2e9d926dafaba1053a979fda0014e",
     "grade": false,
     "grade_id": "Task7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def correct_spelling(word, vocabulary):\n",
    "    # YOUR CODE HERE\n",
    "    n = len(vocabulary)\n",
    "    distances = [None]*n\n",
    "    for i in range(n):\n",
    "        correct_word = vocabulary[i]\n",
    "        distances[i] = min_edit_distance(word,correct_word)[1]\n",
    "    new_word = vocabulary[np.argmin(distances)]\n",
    "    return new_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adafe663",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90d4b9ff3121a5e8cda3dd37794ca7d7",
     "grade": true,
     "grade_id": "Task7b_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert correct_spelling(\"abilitey\",vocabulary) == \"ability\", \"Task 7b test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf244561",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a14be2fbc39d8ace3f511358ab4ffa7e",
     "grade": true,
     "grade_id": "Task7b_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert correct_spelling(\"annonsment\",vocabulary) == \"announcement\", \"Task 7b test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ff5a71f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a712807b78332ae7cf92d094502e997",
     "grade": true,
     "grade_id": "Task7b_3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(s) passed\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE/FUNCTION\n",
    "assert correct_spelling(\"cheet\",vocabulary) == \"cheat\", \"Task 7b test(s) failed\"\n",
    "print(\"Test(s) passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71923e5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4069126af0e8990c34454648706869d5",
     "grade": true,
     "grade_id": "Task7b_4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "383ce288",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45e40c695c8001e882f22ba50cf955e3",
     "grade": true,
     "grade_id": "Task7b_5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c032c4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb6f84c500362d08e42b4926f61442cf",
     "grade": true,
     "grade_id": "Task7b_6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b70ab25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fc8a0762851cf83d569cd6b1bc7e0eb",
     "grade": true,
     "grade_id": "Task7b_7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#IGNORE THIS CELL, PERFORMING BACKGROUND TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c34f9006-5c92-4e8b-bf3f-1999839c712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abilitey ability  \n",
      "abouy about  \n",
      "absorbtion absorption  \n",
      "accidently accidentally  \n",
      "accomodate accommodate  \n",
      "accosinly daily  \n",
      "acommadate accommodate  \n",
      "acord accord  \n",
      "adultry adultery  \n",
      "aggresive aggressive  \n",
      "alchohol alcohol  \n",
      "alchoholic alcoholic  \n",
      "allieve alive  \n",
      "amature amateur  \n",
      "ambivilant ambivalent  \n",
      "amification amplification  \n",
      "amourfous amorphous  \n",
      "annoint anoint  \n",
      "annonsment announcement  \n",
      "annuncio announce  \n",
      "anonomy anatomy  \n",
      "anotomy anatomy  \n",
      "anynomous anonymous  \n",
      "appreceiated appreciated  \n",
      "appresteate appreciate  \n",
      "aquantance acquaintance  \n",
      "aratictature architecture  \n",
      "archeype archetype  \n",
      "aricticure arctic  \n",
      "artic arctic  \n",
      "asentote asymptote  \n",
      "ast at  \n",
      "asterick asterisk  \n",
      "asymetric asymmetric  \n",
      "atentively attentively  \n",
      "autoamlly anatomy  \n",
      "bankrot bankrupt  \n",
      "basicly basically  \n",
      "batallion battalion  \n",
      "bbrose browse  \n",
      "beauro bureau  \n",
      "beaurocracy bureaucracy  \n",
      "beggining beginning  \n",
      "beging beginning  \n",
      "behavior behavior  \n",
      "behaviour behaviour  \n",
      "beleive believe  \n",
      "belive believe  \n",
      "benidifs benefits  \n",
      "bigginging beginning  \n",
      "blait bleat  \n",
      "bouyant buoyant  \n",
      "boygot boycott  \n",
      "brocolli broccoli  \n",
      "buch bush  \n",
      "buder butter  \n",
      "budr bureau  \n",
      "budter butter  \n",
      "buracracy bureaucracy  \n",
      "burracracy bureaucracy  \n",
      "buton button  \n",
      "cauler caller  \n",
      "cemetary cemetery  \n",
      "changeing changing  \n",
      "cheet cheat  \n",
      "cicle circle  \n",
      "cimplicity simplicity  \n",
      "circue circle  \n",
      "circumstaces circumstances  \n",
      "clob club  \n",
      "coaln colon  \n",
      "cocamena came  \n",
      "colleaque colleague  \n",
      "colloquilism colloquialism  \n",
      "columne column  \n",
      "comiler compiler  \n",
      "comitmment commitment  \n",
      "comitte committed  \n",
      "comittmen commitment  \n",
      "comittmend committed  \n",
      "commerciasl commercials  \n",
      "commited committed  \n",
      "commitee committee  \n",
      "companys companies  \n",
      "compicated complicated  \n",
      "comupter computer  \n",
      "concensus consensus  \n",
      "congradulations congratulations  \n",
      "conibation contribution  \n",
      "consident consistent  \n",
      "consident consistent  \n",
      "contast constant  \n",
      "contastant constant  \n",
      "contunie continue  \n",
      "cooly coolly  \n",
      "copping coping  \n",
      "cosmoplyton cosmopolitan  \n",
      "courst court  \n",
      "crasy crazy  \n",
      "cravets caveats  \n",
      "credetability credibility  \n",
      "criqitue critique  \n",
      "croke croak  \n",
      "crucifiction crucifixion  \n",
      "crusifed crucified  \n",
      "ctitique critique  \n",
      "cumba club  \n",
      "custamisation customization  \n",
      "dag dog  \n",
      "daly daily  \n",
      "danguages dangle  \n",
      "deaft draft  \n",
      "defence defence  \n",
      "defenly defence  \n",
      "defense defense  \n",
      "definate definite  \n",
      "definately definitely  \n",
      "dependeble dependable  \n",
      "descrption description  \n",
      "descrptn description  \n",
      "desparate desperate  \n",
      "dessicate desiccate  \n",
      "destint distant  \n",
      "develepment development  \n",
      "developement development  \n",
      "develpond development  \n",
      "devulge divulge  \n",
      "diagree disagree  \n",
      "dieties deities  \n",
      "dinasaur dinosaur  \n",
      "dinasour dinosaur  \n",
      "direcyly directly  \n",
      "discuess discuss  \n",
      "disect dissect  \n",
      "disippate dissipate  \n",
      "disition decision  \n",
      "dispair despair  \n",
      "disssicion discussion  \n",
      "distarct distract  \n",
      "distart distant  \n",
      "distroy destroy  \n",
      "documtations documentation  \n",
      "doenload download  \n",
      "dongle dangle  \n",
      "doog dog  \n",
      "dramaticly dramatically  \n",
      "drunkeness drunkenness  \n",
      "ductioneery dictionary  \n",
      "dur due  \n",
      "duren due  \n",
      "dymatic dynamic  \n",
      "dynaic dynamic  \n",
      "ecstacy ecstasy  \n",
      "efficat efficacy  \n",
      "efficity efficient  \n",
      "effots efforts  \n",
      "egsistence existence  \n",
      "eitiology etiology  \n",
      "elagent elegant  \n",
      "elligit height  \n",
      "embarass embarrass  \n",
      "embarassment embarrassment  \n",
      "embaress embarrass  \n",
      "encapsualtion encapsulation  \n",
      "encyclapidia encyclopedia  \n",
      "encyclopia encyclopedia  \n",
      "engins engine  \n",
      "enhence enhance  \n",
      "enligtment enlightenment  \n",
      "ennuui ennui  \n",
      "enought enough  \n",
      "enventions inventions  \n",
      "envireminakl environmental  \n",
      "enviroment environment  \n",
      "epitomy epitome  \n",
      "equire acquire  \n",
      "errara error  \n",
      "erro error  \n",
      "evaualtion evaluation  \n",
      "evething everything  \n",
      "evtually eventually  \n",
      "excede exceed  \n",
      "excercise exercise  \n",
      "excpt except  \n",
      "excution execution  \n",
      "exhileration exhilaration  \n",
      "existance existence  \n",
      "expleyly explicitly  \n",
      "explity explicitly  \n",
      "expresso espresso  \n",
      "exspidient expedient  \n",
      "extions extensions  \n",
      "factontion factorization  \n",
      "failer failure  \n",
      "famdasy fantasy  \n",
      "faver favor  \n",
      "faver favor  \n",
      "faxe fax  \n",
      "firey fiery  \n",
      "fistival festival  \n",
      "flatterring flattering  \n",
      "fluk flux  \n",
      "flukse flux  \n",
      "fone phone  \n",
      "forsee foresee  \n",
      "frustartaion frustrating  \n",
      "fuction function  \n",
      "funetik function  \n",
      "futs guts  \n",
      "gamne came  \n",
      "gaurd guard  \n",
      "generly generally  \n",
      "goberment government  \n",
      "gobernement government  \n",
      "gobernment government  \n",
      "gotton gotten  \n",
      "gracefull graceful  \n",
      "gradualy gradually  \n",
      "grammer grammar  \n",
      "hallo hello  \n",
      "hapily happily  \n",
      "harrass harass  \n",
      "havne have  \n",
      "heellp help  \n",
      "heighth height  \n",
      "hellp help  \n",
      "helo hello  \n",
      "herlo hello  \n",
      "hifin john  \n",
      "hifine fiery  \n",
      "higer higher  \n",
      "hiphine phone  \n",
      "hippopotamous hippopotamus  \n",
      "hlp help  \n",
      "hourse horse  \n",
      "houssing housing  \n",
      "howaver however  \n",
      "howver however  \n",
      "humaniti humanity  \n",
      "hyfin hyphen  \n",
      "hypotathes hypothesis  \n",
      "hypotathese hypothesis  \n",
      "hystrical hysterical  \n",
      "ident indent  \n",
      "illegitament illegitimate  \n",
      "imbed embed  \n",
      "imediaetly immediately  \n",
      "imfamy infamy  \n",
      "immenant immanent  \n",
      "implemtes implements  \n",
      "inadvertant inadvertent  \n",
      "incedious insidious  \n",
      "incompleet incomplete  \n",
      "incomplot incomplete  \n",
      "inconvenant inconvenient  \n",
      "inconvience inconvenience  \n",
      "independant independent  \n",
      "independenent independent  \n",
      "indepnends independent  \n",
      "indispensible indispensable  \n",
      "inefficite inefficient  \n",
      "inerface interface  \n",
      "influencial influential  \n",
      "inital initial  \n",
      "initinized initialized  \n",
      "initized initialized  \n",
      "innoculate inoculate  \n",
      "insistant insistent  \n",
      "insistenet insistent  \n",
      "instulation installation  \n",
      "intealignt intelligent  \n",
      "intejilent intelligent  \n",
      "intelegent intelligent  \n",
      "intelegnent intelligent  \n",
      "intelejent intelligent  \n",
      "inteligent intelligent  \n",
      "intelignt intelligent  \n",
      "intellagant intelligent  \n",
      "intellegent intelligent  \n",
      "intellegint intelligent  \n",
      "intellgnt intelligent  \n",
      "interate iterate  \n",
      "internation international  \n",
      "interpretate interpret  \n",
      "interpretter interpreter  \n",
      "intertes iterate  \n",
      "intertesd interested  \n",
      "invermeantial environmental  \n",
      "irregardless regardless  \n",
      "irresistable irresistible  \n",
      "irritible irritable  \n",
      "isotrop isotope  \n",
      "johhn john  \n",
      "judgement judgement  \n",
      "judgment judgment  \n",
      "kippur kipper  \n",
      "knawing knowing  \n",
      "latext latest  \n",
      "leasve leave  \n",
      "lesure leisure  \n",
      "liasion liaison  \n",
      "liason liaison  \n",
      "libary library  \n",
      "likly likely  \n",
      "lilometer kilometer  \n",
      "lilometer kilometer  \n",
      "liquify liquefy  \n",
      "lloyer layer  \n",
      "lossing losing  \n",
      "luser laser  \n",
      "maddness madness  \n",
      "maintanence maintenance  \n",
      "majaerly majority  \n",
      "majoraly majority  \n",
      "maks masks  \n",
      "mant at  \n",
      "marshall marshal  \n",
      "maxium maximum  \n",
      "meory memory  \n",
      "metter better  \n",
      "mic mike  \n",
      "midia media  \n",
      "millenium millennium  \n",
      "miniscule minuscule  \n",
      "minkay infamy  \n",
      "minum minimum  \n",
      "mischievious mischievous  \n",
      "misilous insidious  \n",
      "momento memento  \n",
      "monkay monkey  \n",
      "mosaik mosaic  \n",
      "mousr mouser  \n",
      "mroe more  \n",
      "neccessary necessary  \n",
      "necesary necessary  \n",
      "necesser necessary  \n",
      "neice niece  \n",
      "neighbor neighbor  \n",
      "neighbour neighbour  \n",
      "nickleodeon nickelodeon  \n",
      "nieve naive  \n",
      "noticably noticeably  \n",
      "nozled nuzzled  \n",
      "objectsion objects  \n",
      "obsfuscate obfuscate  \n",
      "ocassion occasion  \n",
      "occuppied occupied  \n",
      "occurence occurrence  \n",
      "occusionaly occasionally  \n",
      "octagenarian octogenarian  \n",
      "olf old  \n",
      "opposim opossum  \n",
      "organise organize  \n",
      "organiz organize  \n",
      "orientate orient  \n",
      "oscilascope oscilloscope  \n",
      "oving moving  \n",
      "paramers parameters  \n",
      "parametic parameter  \n",
      "paranets parameters  \n",
      "partrucal portray  \n",
      "pataphysical metaphysical  \n",
      "patten pattern  \n",
      "permissable permissible  \n",
      "permition permission  \n",
      "permmasivie permissive  \n",
      "perogative prerogative  \n",
      "persue pursue  \n",
      "phantasia fantasia  \n",
      "phenominal phenomenal  \n",
      "picaresque picturesque  \n",
      "playwrite playwright  \n",
      "polation colon  \n",
      "poligamy polygamy  \n",
      "politict politic  \n",
      "pollice police  \n",
      "polypropalene polypropylene  \n",
      "possable possible  \n",
      "practicle practical  \n",
      "pragmaticism pragmatism  \n",
      "preceeding preceding  \n",
      "precion precision  \n",
      "precios precision  \n",
      "preemptory peremptory  \n",
      "prefices prefixes  \n",
      "prefixt prefixed  \n",
      "presue pursue  \n",
      "presued pursued  \n",
      "privielage privilege  \n",
      "priviledge privilege  \n",
      "proceedures procedures  \n",
      "pronensiation pronunciation  \n",
      "pronisation pronunciation  \n",
      "pronounciation pronunciation  \n",
      "properally properly  \n",
      "proplematic problematic  \n",
      "protray portray  \n",
      "pscolgst psychologist  \n",
      "psicolagest latest  \n",
      "psycolagest psychologist  \n",
      "quoz quiz  \n",
      "radious radius  \n",
      "ramplily daily  \n",
      "reccomend recommend  \n",
      "reccona raccoon  \n",
      "recieve receive  \n",
      "reconise recognize  \n",
      "rectangeles rectangle  \n",
      "redign redesign  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reoccurring recurring  \n",
      "repitition repetition  \n",
      "replasments replacement  \n",
      "reposable responsible  \n",
      "reseblence resemblance  \n",
      "respct respect  \n",
      "respecally respectfully  \n",
      "roon room  \n",
      "rought roughly  \n",
      "rudemtry destroy  \n",
      "runnung running  \n",
      "sacreligious sacrilegious  \n",
      "saftly safely  \n",
      "salut salute  \n",
      "satifly satisfy  \n",
      "scrabdle scrabble  \n",
      "secion section  \n",
      "seferal several  \n",
      "segements segments  \n",
      "sence sense  \n",
      "seperate separate  \n",
      "sherbert sherbet  \n",
      "sicolagest latest  \n",
      "sieze seize  \n",
      "simpfilty simply  \n",
      "simplye simply  \n",
      "singal signal  \n",
      "sitte site  \n",
      "situration situation  \n",
      "slyph sylph  \n",
      "smil smile  \n",
      "snuck sonic  \n",
      "sometmes sometimes  \n",
      "soonec sonic  \n",
      "specificialy specifically  \n",
      "spel spell  \n",
      "spoak spoke  \n",
      "sponsered sponsored  \n",
      "stering steering  \n",
      "straightjacket straitjacket  \n",
      "stumach stomach  \n",
      "stutent student  \n",
      "subisitions substitutions  \n",
      "subjecribed subscribed  \n",
      "subpena subpoena  \n",
      "substations substitutions  \n",
      "suger sugar  \n",
      "supercede supersede  \n",
      "superfulous superfluous  \n",
      "syncorization synchronization  \n",
      "taff at  \n",
      "taht at  \n",
      "tattos tattoos  \n",
      "techniquely technically  \n",
      "teh the  \n",
      "tem team  \n",
      "teo the  \n",
      "teridical hysterical  \n",
      "tesst test  \n",
      "tets tests  \n",
      "thanot that  \n",
      "theirselves themselves  \n",
      "theridically theoretically  \n",
      "thredically theoretically  \n",
      "thruout throughout  \n",
      "ths this  \n",
      "thw the  \n",
      "titalate titillate  \n",
      "tommorrow tomorrow  \n",
      "tomorow tomorrow  \n",
      "tradegy tragedy  \n",
      "trubbel trouble  \n",
      "ttest test  \n",
      "tured turned  \n",
      "tyrrany tyranny  \n",
      "unatourral unnatural  \n",
      "unaturral unnatural  \n",
      "unconisitional unconstitutional  \n",
      "unconscience inconvenience  \n",
      "unentelegible unintelligible  \n",
      "unformanlly generally  \n",
      "unfortally unfortunately  \n",
      "unfortunently unfortunately  \n",
      "unnaturral unnatural  \n",
      "upmost utmost  \n",
      "uranisium uranium  \n",
      "verison version  \n",
      "vinagarette vinaigrette  \n",
      "volumptuous voluptuous  \n",
      "volunteerism voluntarism  \n",
      "volye volley  \n",
      "wadting wasting  \n",
      "waite wait  \n",
      "wan't want  \n",
      "warloord warlord  \n",
      "whaaat what  \n",
      "whard ward  \n",
      "whimp wimp  \n",
      "wicken mike  \n",
      "wierd weird  \n",
      "wrank rank  \n",
      "writting writing  \n",
      "yeild yield  \n"
     ]
    }
   ],
   "source": [
    "for incorrect_word in incorrect_words:\n",
    "    print(incorrect_word, correct_spelling(incorrect_word,vocabulary), \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc1679-dccc-43ca-9466-cb809b353a3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Some thoughts\n",
    "What do you think about this spell checker?\n",
    "- The spell checker performs relatively well. Even though certain words suggested are incorrect, majority of the suggestions are correct and make sense.\n",
    "\n",
    "\n",
    "Did you identify any suggestion that seems incorrect?\n",
    "- Yes, some of these words are outlined in <b>Task 7b</b>. <br>\n",
    "\n",
    "Can you identify any drawbacks? \n",
    "- A correct word that is not in the dictionary will never be suggested.\n",
    "- We also iterate through the entire vocabulary and then find the minimum edit distance. This is inefficient.\n",
    "\n",
    "For each drawback identified can you think of a solution?\n",
    "- If we increase the size of the dictionary, more apt words that are not in the dictionary can be suggested.\n",
    "- We can use BK-Trees, which will be eliminate the need to iterate through the entire vocabulary for each spell check.\n",
    "\n",
    "##### Hints\n",
    "Checking through an entire vocabulary is computationally expensive, perhaps it can be improved to only check a subset of words.\n",
    "Also, no context of the word is factored in, what if we have the word 'iat', its edit distance with 'bat', 'cat', 'mat', and  'eat' are all the same. How can one make a choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb4ec6-cca4-4fd7-b3ba-5e03739b08ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "855758f989a22533df79160226c1d419",
     "grade": false,
     "grade_id": "cell-c551a6fa23bebd65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 7b (Ungraded -   for learning purposes)\n",
    "\n",
    "List some suggested words that you think are incorrect. Give the incorrect word, the suggested word based on the minimum edit distance and the most appropriate word from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf5590",
   "metadata": {},
   "source": [
    "- Incorrect word: defenly   Correct word given: defence  Appropriate word: definitely\n",
    "- Incorrect word: hifin   Correct word given: john  Appropriate word: hyphen\n",
    "- Incorrect word: hifine   Correct word given: fiery  Appropriate word: hyphen\n",
    "- Incorrect word: misilous   Correct word given: insiduous  Appropriate word: miscellaneous\n",
    "- Incorrect word: rudemtry   Correct word given: destroy  Appropriate word: rudimentary\n",
    "- Incorrect word: snuck   Correct word given: sonic  Appropriate word: sneaked\n",
    "- Incorrect word: taht   Correct word given: at  Appropriate word: that\n",
    "- Incorrect word: uncoscience   Correct word given: inconvenience  Appropriate word: unconscious\n",
    "- Incorrect word: unformanlly   Correct word given: generally  Appropriate word: unfortunately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c93b7-b520-41a6-aa7f-381143950321",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88dd7c02e4b7893d73817c2cff9f8358",
     "grade": false,
     "grade_id": "cell-ccb2cba35167b685",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 8  ( (Ungraded -   for learning purposes)\n",
    "Based on any of your identified drawbacks, create another spell checker that is an improvement on the one built in Task 7.\n",
    "Is it able to give a better suggestion of any of the words identified in Task 7b?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec577e",
   "metadata": {},
   "source": [
    "- I could not think of a manner to improve this current spell checker. However, I did some research into BK-Trees. \n",
    "- In BK-Trees, we build a tree of our vocabulary and suggest words that have an edit distance less than or equal to a defined threshold. \n",
    "- However, this will return multiple possible replacements for a word and not a single correct word.\n",
    "- The BK-Tree solves the problem of inefficeincy of the current spell checker , and cannot solve the problem of giving a more appropriate word, since it is also built using the edit distance like the spell checker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671fe89-f122-455b-a5f4-dc6968ced812",
   "metadata": {},
   "source": [
    "### End of Lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
